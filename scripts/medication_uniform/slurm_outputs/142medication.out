2024-05-09 15:52:27 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 142 ---------------
2024-05-09 15:52:27 INFO     Contexts were splited into 402 paragraphs, which are 2.19672131147541 paragraphs on average per one report. The overall paragraph average length (characters) is 2993.2213930348257
2024-05-09 15:52:27 INFO     Contexts were splited into 66 paragraphs, which are 2.5384615384615383 paragraphs on average per one report. The overall paragraph average length (characters) is 2744.151515151515
2024-05-09 15:52:27 INFO     Contexts were splited into 113 paragraphs, which are 2.1320754716981134 paragraphs on average per one report. The overall paragraph average length (characters) is 2999.8407079646017
2024-05-09 15:52:34 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-09 15:56:02 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-09 17:07:11 INFO     the model is trained
2024-05-09 17:12:33 INFO     evaluation data are prepared
2024-05-09 18:18:26 INFO     QA scores: {'exact_match': 31.465572784674197, 'f1': 73.33260892676597}
2024-05-09 18:18:27 INFO     PR scores: {'p@1': 0.9579485417293072, 'p@2': 0.9979811863751558, 'p@3': 0.9996778488896525}
2024-05-09 18:18:29 INFO     PRQA scores: {'exact_match': 31.11550191142992, 'f1': 71.93208814681212}
{'loss': 1.1065, 'grad_norm': 6.738910675048828, 'learning_rate': 2.88613936541673e-05, 'epoch': 0.037953544861090024}
{'loss': 0.7073, 'grad_norm': 4.491283893585205, 'learning_rate': 2.77227873083346e-05, 'epoch': 0.07590708972218005}
{'loss': 0.6339, 'grad_norm': 10.771079063415527, 'learning_rate': 2.65841809625019e-05, 'epoch': 0.11386063458327007}
{'loss': 0.5533, 'grad_norm': 3.8675620555877686, 'learning_rate': 2.5445574616669196e-05, 'epoch': 0.1518141794443601}
{'loss': 0.5307, 'grad_norm': 6.606520175933838, 'learning_rate': 2.4306968270836497e-05, 'epoch': 0.18976772430545014}
{'loss': 0.4615, 'grad_norm': 10.175904273986816, 'learning_rate': 2.3168361925003797e-05, 'epoch': 0.22772126916654015}
{'loss': 0.4595, 'grad_norm': 10.19192123413086, 'learning_rate': 2.2029755579171094e-05, 'epoch': 0.26567481402763016}
{'loss': 0.4021, 'grad_norm': 8.107645034790039, 'learning_rate': 2.0891149233338395e-05, 'epoch': 0.3036283588887202}
{'loss': 0.3691, 'grad_norm': 12.093838691711426, 'learning_rate': 1.9752542887505692e-05, 'epoch': 0.34158190374981023}
{'loss': 0.366, 'grad_norm': 4.25016975402832, 'learning_rate': 1.8613936541672992e-05, 'epoch': 0.3795354486109003}
{'loss': 0.3183, 'grad_norm': 13.705543518066406, 'learning_rate': 1.747533019584029e-05, 'epoch': 0.4174889934719903}
{'loss': 0.3001, 'grad_norm': 30.05689811706543, 'learning_rate': 1.633672385000759e-05, 'epoch': 0.4554425383330803}
{'loss': 0.2812, 'grad_norm': 13.435218811035156, 'learning_rate': 1.5198117504174889e-05, 'epoch': 0.49339608319417033}
{'loss': 0.2713, 'grad_norm': 11.11630630493164, 'learning_rate': 1.405951115834219e-05, 'epoch': 0.5313496280552603}
{'loss': 0.2379, 'grad_norm': 19.662200927734375, 'learning_rate': 1.2920904812509488e-05, 'epoch': 0.5693031729163504}
{'loss': 0.2424, 'grad_norm': 15.330941200256348, 'learning_rate': 1.1782298466676789e-05, 'epoch': 0.6072567177774404}
{'loss': 0.2056, 'grad_norm': 4.065054893493652, 'learning_rate': 1.0643692120844087e-05, 'epoch': 0.6452102626385304}
{'loss': 0.2152, 'grad_norm': 7.598006725311279, 'learning_rate': 9.505085775011386e-06, 'epoch': 0.6831638074996205}
{'loss': 0.1996, 'grad_norm': 1.9110321998596191, 'learning_rate': 8.366479429178685e-06, 'epoch': 0.7211173523607105}
{'loss': 0.1971, 'grad_norm': 3.3682680130004883, 'learning_rate': 7.227873083345985e-06, 'epoch': 0.7590708972218005}
{'loss': 0.1809, 'grad_norm': 0.9694465398788452, 'learning_rate': 6.0892667375132836e-06, 'epoch': 0.7970244420828906}
{'loss': 0.1793, 'grad_norm': 12.403587341308594, 'learning_rate': 4.950660391680583e-06, 'epoch': 0.8349779869439806}
{'loss': 0.1611, 'grad_norm': 22.58620834350586, 'learning_rate': 3.812054045847882e-06, 'epoch': 0.8729315318050705}
{'loss': 0.1464, 'grad_norm': 16.571372985839844, 'learning_rate': 2.6734477000151817e-06, 'epoch': 0.9108850766661606}
{'loss': 0.1461, 'grad_norm': 1.0617504119873047, 'learning_rate': 1.5348413541824807e-06, 'epoch': 0.9488386215272506}
{'loss': 0.1416, 'grad_norm': 4.746211528778076, 'learning_rate': 3.962350083497799e-07, 'epoch': 0.9867921663883407}
{'eval_loss': 1.1256637573242188, 'eval_runtime': 711.5012, 'eval_samples_per_second': 179.318, 'eval_steps_per_second': 0.701, 'epoch': 1.0}
{'train_runtime': 4268.096, 'train_samples_per_second': 49.383, 'train_steps_per_second': 3.087, 'train_loss': 0.34391468566991823, 'epoch': 1.0}
Post-processing 110915 example predictions split into 390292 features.
{
    "142": {
        "QA": {
            "exact_match": 31.465572784674197,
            "f1": 73.33260892676597
        },
        "PR": {
            "p@1": 0.9579485417293072,
            "p@2": 0.9979811863751558,
            "p@3": 0.9996778488896525
        },
        "PRQA": {
            "exact_match": 31.11550191142992,
            "f1": 71.93208814681212
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 702882 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 702882 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
