2024-05-07 12:33:47 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 100 ---------------
2024-05-07 12:33:47 INFO     Contexts were splited into 787 paragraphs, which are 4.300546448087432 paragraphs on average per one report. The overall paragraph average length (characters) is 1528.9390088945363
2024-05-07 12:33:47 INFO     Contexts were splited into 119 paragraphs, which are 4.576923076923077 paragraphs on average per one report. The overall paragraph average length (characters) is 1521.9663865546217
2024-05-07 12:33:48 INFO     Contexts were splited into 218 paragraphs, which are 4.113207547169812 paragraphs on average per one report. The overall paragraph average length (characters) is 1554.9633027522937
2024-05-07 12:33:55 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-07 12:36:01 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-07 13:17:58 INFO     the model is trained
2024-05-07 13:23:26 INFO     evaluation data are prepared
