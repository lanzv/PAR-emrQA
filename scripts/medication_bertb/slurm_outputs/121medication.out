2024-05-09 18:03:39 INFO     ------------- Experiment: model BERTbase, frequency threshold 121 ---------------
2024-05-09 18:03:40 INFO     Contexts were splited into 4977 sentence groups, which are 27.19672131147541 groups on average per one report
2024-05-09 18:03:40 INFO     Contexts were splited into 552 paragraphs, which are 3.0163934426229506 paragraphs on average per one report. There are 2 unique topics with frequency threshold (greater or equal) 121. The overall paragraph average length (characters) is 2212.0072463768115
2024-05-09 18:03:40 INFO     Contexts were splited into 818 sentence groups, which are 31.46153846153846 groups on average per one report
2024-05-09 18:03:41 INFO     Contexts were splited into 44 paragraphs, which are 1.6923076923076923 paragraphs on average per one report. There are 2 unique topics with frequency threshold (greater or equal) 121. The overall paragraph average length (characters) is 4164.931818181818
2024-05-09 18:03:41 INFO     Contexts were splited into 1362 sentence groups, which are 25.69811320754717 groups on average per one report
2024-05-09 18:03:42 WARNING  The evidenc 'CEFPODOXIME PROXETIL 200 MG PO BID X 16 doses
Food/Drug Interaction Instruction Give with meals
' is in wrong format and it does not occure in the paragraph 'POTENTIALLY SERIOUS INTERACTION: WARFARIN &amp; SIMVASTATIN
Reason for override: already tolerated , home medication ,
aware
CEFPODOXIME PROXETIL 200 MG PO BID X 16 doses
Starting Today ( 6/25 ) HOLD IF: rash
Food/Drug Interaction Instruction Give with meals
Alert overridden: Override added on 11/4/05 by :
on order for CEFPODOXIME PROXETIL PO ( ref # 703442345 )
Pt. has a POSSIBLE allergy to Penicillins; reaction is
RASH. Reason for override: cannot tolerate floroquinolones
and needs gram neg coverage. has
tolerated cefalosporins well in the past.
'
2024-05-09 18:03:42 INFO     Contexts were splited into 148 paragraphs, which are 2.792452830188679 paragraphs on average per one report. There are 2 unique topics with frequency threshold (greater or equal) 121. The overall paragraph average length (characters) is 2323.0337837837837
2024-05-09 18:03:51 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-09 18:08:02 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-09 19:38:32 INFO     the model is trained
2024-05-09 19:44:21 INFO     evaluation data are prepared
2024-05-09 21:02:54 INFO     QA scores: {'exact_match': 29.06017782741291, 'f1': 69.19922781697841}
2024-05-09 21:02:54 INFO     PR scores: {'p@1': 0.9745930157639277, 'p@2': 0.9934066406082213, 'p@3': 0.9975731283020489}
2024-05-09 21:02:56 INFO     PRQA scores: {'exact_match': 28.409432584510974, 'f1': 68.17766662483714}
{'loss': 1.1291, 'grad_norm': 19.535499572753906, 'learning_rate': 2.906477959972567e-05, 'epoch': 0.03117401334247771}
{'loss': 0.7751, 'grad_norm': 5.392856597900391, 'learning_rate': 2.8129559199451336e-05, 'epoch': 0.06234802668495542}
{'loss': 0.6793, 'grad_norm': 12.868309020996094, 'learning_rate': 2.7194338799177008e-05, 'epoch': 0.09352204002743313}
{'loss': 0.6245, 'grad_norm': 7.384043216705322, 'learning_rate': 2.6259118398902675e-05, 'epoch': 0.12469605336991084}
{'loss': 0.5715, 'grad_norm': 5.56233549118042, 'learning_rate': 2.5323897998628343e-05, 'epoch': 0.15587006671238857}
{'loss': 0.5255, 'grad_norm': 6.769736289978027, 'learning_rate': 2.438867759835401e-05, 'epoch': 0.18704408005486625}
{'loss': 0.5152, 'grad_norm': 18.05019760131836, 'learning_rate': 2.3453457198079682e-05, 'epoch': 0.21821809339734397}
{'loss': 0.484, 'grad_norm': 21.025693893432617, 'learning_rate': 2.251823679780535e-05, 'epoch': 0.24939210673982168}
{'loss': 0.4574, 'grad_norm': 9.361083984375, 'learning_rate': 2.1583016397531018e-05, 'epoch': 0.2805661200822994}
{'loss': 0.433, 'grad_norm': 13.523472785949707, 'learning_rate': 2.0647795997256685e-05, 'epoch': 0.31174013342477713}
{'loss': 0.4071, 'grad_norm': 7.174539089202881, 'learning_rate': 1.9712575596982357e-05, 'epoch': 0.3429141467672548}
{'loss': 0.3581, 'grad_norm': 10.392641067504883, 'learning_rate': 1.8777355196708024e-05, 'epoch': 0.3740881601097325}
{'loss': 0.3408, 'grad_norm': 0.2750544548034668, 'learning_rate': 1.7842134796433692e-05, 'epoch': 0.4052621734522102}
{'loss': 0.3424, 'grad_norm': 4.845517158508301, 'learning_rate': 1.690691439615936e-05, 'epoch': 0.43643618679468793}
{'loss': 0.333, 'grad_norm': 6.727099895477295, 'learning_rate': 1.597169399588503e-05, 'epoch': 0.46761020013716564}
{'loss': 0.3135, 'grad_norm': 4.222073078155518, 'learning_rate': 1.5036473595610697e-05, 'epoch': 0.49878421347964336}
{'loss': 0.2925, 'grad_norm': 16.352930068969727, 'learning_rate': 1.4101253195336368e-05, 'epoch': 0.5299582268221211}
{'loss': 0.2789, 'grad_norm': 8.787212371826172, 'learning_rate': 1.3166032795062036e-05, 'epoch': 0.5611322401645988}
{'loss': 0.2708, 'grad_norm': 12.309874534606934, 'learning_rate': 1.2230812394787706e-05, 'epoch': 0.5923062535070766}
{'loss': 0.251, 'grad_norm': 0.9385221004486084, 'learning_rate': 1.1295591994513373e-05, 'epoch': 0.6234802668495543}
{'loss': 0.2466, 'grad_norm': 13.215811729431152, 'learning_rate': 1.0360371594239043e-05, 'epoch': 0.6546542801920319}
{'loss': 0.2337, 'grad_norm': 11.411396980285645, 'learning_rate': 9.42515119396471e-06, 'epoch': 0.6858282935345096}
{'loss': 0.2015, 'grad_norm': 22.766433715820312, 'learning_rate': 8.48993079369038e-06, 'epoch': 0.7170023068769873}
{'loss': 0.1954, 'grad_norm': 6.567208290100098, 'learning_rate': 7.554710393416048e-06, 'epoch': 0.748176320219465}
{'loss': 0.2022, 'grad_norm': 12.143473625183105, 'learning_rate': 6.6194899931417166e-06, 'epoch': 0.7793503335619427}
{'loss': 0.1907, 'grad_norm': 13.13235855102539, 'learning_rate': 5.684269592867386e-06, 'epoch': 0.8105243469044204}
{'loss': 0.2018, 'grad_norm': 1.9629948139190674, 'learning_rate': 4.749049192593055e-06, 'epoch': 0.8416983602468981}
{'loss': 0.1798, 'grad_norm': 14.346576690673828, 'learning_rate': 3.8138287923187233e-06, 'epoch': 0.8728723735893759}
{'loss': 0.1805, 'grad_norm': 19.97048568725586, 'learning_rate': 2.878608392044392e-06, 'epoch': 0.9040463869318536}
{'loss': 0.1716, 'grad_norm': 8.099196434020996, 'learning_rate': 1.9433879917700606e-06, 'epoch': 0.9352204002743313}
{'loss': 0.1659, 'grad_norm': 6.987051486968994, 'learning_rate': 1.0081675914957292e-06, 'epoch': 0.966394413616809}
{'loss': 0.1715, 'grad_norm': 7.142940521240234, 'learning_rate': 7.294719122139784e-08, 'epoch': 0.9975684269592867}
{'eval_loss': 0.7875460386276245, 'eval_runtime': 1085.4926, 'eval_samples_per_second': 179.061, 'eval_steps_per_second': 0.7, 'epoch': 1.0}
{'train_runtime': 5429.6213, 'train_samples_per_second': 47.261, 'train_steps_per_second': 2.954, 'train_loss': 0.36579566766246685, 'epoch': 1.0}
Post-processing 138930 example predictions split into 471816 features.
{
    "121": {
        "QA": {
            "exact_match": 29.06017782741291,
            "f1": 69.19922781697841
        },
        "PR": {
            "p@1": 0.9745930157639277,
            "p@2": 0.9934066406082213,
            "p@3": 0.9975731283020489
        },
        "PRQA": {
            "exact_match": 28.409432584510974,
            "f1": 68.17766662483714
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 2662528 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 2662528 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
