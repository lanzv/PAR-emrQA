2024-05-11 13:56:06 INFO     ------------- Experiment: model BERTbase, frequency threshold 66 ---------------
2024-05-11 13:56:06 INFO     Contexts were splited into 1917 paragraphs, which are 10.475409836065573 paragraphs on average per one report. The overall paragraph average length (characters) is 627.6864893062076
2024-05-11 13:56:07 INFO     Contexts were splited into 289 paragraphs, which are 11.115384615384615 paragraphs on average per one report. The overall paragraph average length (characters) is 626.6920415224913
2024-05-11 13:56:07 INFO     Contexts were splited into 545 paragraphs, which are 10.283018867924529 paragraphs on average per one report. The overall paragraph average length (characters) is 621.9853211009174
2024-05-11 13:56:16 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-11 13:57:20 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-11 14:19:32 INFO     the model is trained
2024-05-11 14:25:18 INFO     evaluation data are prepared
2024-05-11 15:54:44 INFO     QA scores: {'exact_match': 30.462608994459, 'f1': 73.15211278797608}
2024-05-11 15:54:44 INFO     PR scores: {'p@1': 0.8470856062883897, 'p@2': 0.958485460246553, 'p@3': 0.9819595378205404}
2024-05-11 15:54:46 INFO     PRQA scores: {'exact_match': 28.17748378506078, 'f1': 67.00653486186177}
{'loss': 1.903, 'grad_norm': 19.491119384765625, 'learning_rate': 2.6345919610231426e-05, 'epoch': 0.1218026796589525}
{'loss': 1.2772, 'grad_norm': 14.27086353302002, 'learning_rate': 2.269183922046285e-05, 'epoch': 0.243605359317905}
{'loss': 1.0377, 'grad_norm': 16.78412628173828, 'learning_rate': 1.9037758830694276e-05, 'epoch': 0.3654080389768575}
{'loss': 0.8352, 'grad_norm': 10.084585189819336, 'learning_rate': 1.53836784409257e-05, 'epoch': 0.48721071863581}
{'loss': 0.7042, 'grad_norm': 9.9946870803833, 'learning_rate': 1.1729598051157126e-05, 'epoch': 0.6090133982947625}
{'loss': 0.5971, 'grad_norm': 12.793717384338379, 'learning_rate': 8.07551766138855e-06, 'epoch': 0.730816077953715}
{'loss': 0.5526, 'grad_norm': 19.74550437927246, 'learning_rate': 4.4214372716199755e-06, 'epoch': 0.8526187576126675}
{'loss': 0.4701, 'grad_norm': 14.299515724182129, 'learning_rate': 7.673568818514008e-07, 'epoch': 0.97442143727162}
{'eval_loss': 1.8690630197525024, 'eval_runtime': 234.5362, 'eval_samples_per_second': 181.546, 'eval_steps_per_second': 0.712, 'epoch': 1.0}
{'train_runtime': 1330.9425, 'train_samples_per_second': 49.348, 'train_steps_per_second': 3.084, 'train_loss': 0.9099759611810458, 'epoch': 1.0}
Post-processing 534940 example predictions split into 534940 features.
{
    "66": {
        "QA": {
            "exact_match": 30.462608994459,
            "f1": 73.15211278797608
        },
        "PR": {
            "p@1": 0.8470856062883897,
            "p@2": 0.958485460246553,
            "p@3": 0.9819595378205404
        },
        "PRQA": {
            "exact_match": 28.17748378506078,
            "f1": 67.00653486186177
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1213202 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1213202 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
