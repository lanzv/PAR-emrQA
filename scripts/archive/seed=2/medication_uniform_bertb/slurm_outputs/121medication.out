2024-05-11 14:49:07 INFO     ------------- Experiment: model BERTbase, frequency threshold 121 ---------------
2024-05-11 14:49:07 INFO     Contexts were splited into 544 paragraphs, which are 2.9726775956284155 paragraphs on average per one report. The overall paragraph average length (characters) is 2211.9025735294117
2024-05-11 14:49:07 INFO     Contexts were splited into 81 paragraphs, which are 3.1153846153846154 paragraphs on average per one report. The overall paragraph average length (characters) is 2235.9753086419755
2024-05-11 14:49:07 INFO     Contexts were splited into 149 paragraphs, which are 2.811320754716981 paragraphs on average per one report. The overall paragraph average length (characters) is 2275.046979865772
2024-05-11 14:49:15 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-11 14:52:13 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-11 15:55:43 INFO     the model is trained
2024-05-11 16:01:15 INFO     evaluation data are prepared
2024-05-11 17:15:11 INFO     QA scores: {'exact_match': 28.533997680512005, 'f1': 69.56690957068727}
2024-05-11 17:15:11 INFO     PR scores: {'p@1': 0.9262918259524935, 'p@2': 0.9929126755723552, 'p@3': 0.9990335466689575}
2024-05-11 17:15:14 INFO     PRQA scores: {'exact_match': 27.80593617112667, 'f1': 67.16128517228869}
{'loss': 1.3423, 'grad_norm': 30.62969207763672, 'learning_rate': 2.873439082011475e-05, 'epoch': 0.042186972662841715}
{'loss': 0.9535, 'grad_norm': 13.180061340332031, 'learning_rate': 2.74687816402295e-05, 'epoch': 0.08437394532568343}
{'loss': 0.7851, 'grad_norm': 10.217338562011719, 'learning_rate': 2.6203172460344247e-05, 'epoch': 0.12656091798852515}
{'loss': 0.712, 'grad_norm': 10.954005241394043, 'learning_rate': 2.4937563280458994e-05, 'epoch': 0.16874789065136686}
{'loss': 0.6541, 'grad_norm': 10.562568664550781, 'learning_rate': 2.3671954100573745e-05, 'epoch': 0.21093486331420858}
{'loss': 0.6153, 'grad_norm': 9.842373847961426, 'learning_rate': 2.2406344920688493e-05, 'epoch': 0.2531218359770503}
{'loss': 0.5551, 'grad_norm': 38.189701080322266, 'learning_rate': 2.1140735740803243e-05, 'epoch': 0.295308808639892}
{'loss': 0.511, 'grad_norm': 3.403871774673462, 'learning_rate': 1.9875126560917987e-05, 'epoch': 0.3374957813027337}
{'loss': 0.4567, 'grad_norm': 27.56787872314453, 'learning_rate': 1.860951738103274e-05, 'epoch': 0.37968275396557544}
{'loss': 0.4292, 'grad_norm': 14.890607833862305, 'learning_rate': 1.7343908201147486e-05, 'epoch': 0.42186972662841715}
{'loss': 0.4126, 'grad_norm': 9.48171615600586, 'learning_rate': 1.6078299021262237e-05, 'epoch': 0.46405669929125887}
{'loss': 0.3628, 'grad_norm': 9.355840682983398, 'learning_rate': 1.4812689841376983e-05, 'epoch': 0.5062436719541006}
{'loss': 0.3488, 'grad_norm': 8.260082244873047, 'learning_rate': 1.3547080661491732e-05, 'epoch': 0.5484306446169422}
{'loss': 0.3383, 'grad_norm': 16.108814239501953, 'learning_rate': 1.228147148160648e-05, 'epoch': 0.590617617279784}
{'loss': 0.3092, 'grad_norm': 11.156474113464355, 'learning_rate': 1.1015862301721228e-05, 'epoch': 0.6328045899426257}
{'loss': 0.3013, 'grad_norm': 6.0561747550964355, 'learning_rate': 9.750253121835978e-06, 'epoch': 0.6749915626054674}
{'loss': 0.2814, 'grad_norm': 13.678719520568848, 'learning_rate': 8.484643941950725e-06, 'epoch': 0.7171785352683091}
{'loss': 0.2688, 'grad_norm': 22.046571731567383, 'learning_rate': 7.219034762065474e-06, 'epoch': 0.7593655079311509}
{'loss': 0.2581, 'grad_norm': 1.081694483757019, 'learning_rate': 5.9534255821802235e-06, 'epoch': 0.8015524805939925}
{'loss': 0.2371, 'grad_norm': 7.869625568389893, 'learning_rate': 4.687816402294972e-06, 'epoch': 0.8437394532568343}
{'loss': 0.2379, 'grad_norm': 15.795875549316406, 'learning_rate': 3.4222072224097198e-06, 'epoch': 0.885926425919676}
{'loss': 0.2143, 'grad_norm': 4.242362976074219, 'learning_rate': 2.1565980425244685e-06, 'epoch': 0.9281133985825177}
{'loss': 0.2201, 'grad_norm': 2.0839221477508545, 'learning_rate': 8.909888626392171e-07, 'epoch': 0.9703003712453594}
{'eval_loss': 1.2392630577087402, 'eval_runtime': 620.4497, 'eval_samples_per_second': 180.221, 'eval_steps_per_second': 0.704, 'epoch': 1.0}
{'train_runtime': 3809.6939, 'train_samples_per_second': 49.775, 'train_steps_per_second': 3.111, 'train_loss': 0.46242696805214345, 'epoch': 1.0}
Post-processing 145864 example predictions split into 437736 features.
{
    "121": {
        "QA": {
            "exact_match": 28.533997680512005,
            "f1": 69.56690957068727
        },
        "PR": {
            "p@1": 0.9262918259524935,
            "p@2": 0.9929126755723552,
            "p@3": 0.9990335466689575
        },
        "PRQA": {
            "exact_match": 27.80593617112667,
            "f1": 67.16128517228869
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 713644 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 713644 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
