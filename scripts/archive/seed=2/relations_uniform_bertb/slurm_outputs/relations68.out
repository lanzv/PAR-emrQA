2024-05-11 20:51:58 INFO     ------------- Experiment: model BERTbase, frequency threshold 68 ---------------
2024-05-11 20:51:59 INFO     Contexts were splited into 2489 paragraphs, which are 8.35234899328859 paragraphs on average per one report. The overall paragraph average length (characters) is 664.7934913619928
2024-05-11 20:51:59 INFO     Contexts were splited into 272 paragraphs, which are 6.476190476190476 paragraphs on average per one report. The overall paragraph average length (characters) is 661.7536764705883
2024-05-11 20:52:00 INFO     Contexts were splited into 637 paragraphs, which are 7.406976744186046 paragraphs on average per one report. The overall paragraph average length (characters) is 662.1302982731554
2024-05-11 20:52:31 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-11 20:54:54 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-11 21:30:34 INFO     the model is trained
2024-05-11 21:54:39 INFO     evaluation data are prepared
2024-05-12 03:14:10 INFO     QA scores: {'exact_match': 91.66104715572467, 'f1': 96.74961387099688}
2024-05-12 03:14:11 INFO     PR scores: {'p@1': 0.9605630749963875, 'p@2': 0.9921848658542459, 'p@3': 0.9961767255912528}
2024-05-12 03:14:21 INFO     PRQA scores: {'exact_match': 88.79088194210298, 'f1': 94.35669585631915}
{'loss': 0.7584, 'grad_norm': 14.120464324951172, 'learning_rate': 2.6656263932233616e-05, 'epoch': 0.11145786892554614}
{'loss': 0.3509, 'grad_norm': 21.531713485717773, 'learning_rate': 2.331252786446723e-05, 'epoch': 0.22291573785109228}
{'loss': 0.2875, 'grad_norm': 17.07147216796875, 'learning_rate': 1.9968791796700847e-05, 'epoch': 0.3343736067766384}
{'loss': 0.2304, 'grad_norm': 3.1698250770568848, 'learning_rate': 1.6625055728934462e-05, 'epoch': 0.44583147570218457}
{'loss': 0.1835, 'grad_norm': 3.098766803741455, 'learning_rate': 1.3281319661168079e-05, 'epoch': 0.5572893446277307}
{'loss': 0.1546, 'grad_norm': 20.105867385864258, 'learning_rate': 9.937583593401694e-06, 'epoch': 0.6687472135532768}
{'loss': 0.1422, 'grad_norm': 14.069384574890137, 'learning_rate': 6.59384752563531e-06, 'epoch': 0.780205082478823}
{'loss': 0.1291, 'grad_norm': 23.509197235107422, 'learning_rate': 3.2501114578689258e-06, 'epoch': 0.8916629514043691}
{'eval_loss': 0.4307663142681122, 'eval_runtime': 936.6511, 'eval_samples_per_second': 180.298, 'eval_steps_per_second': 0.705, 'epoch': 1.0}
{'train_runtime': 2138.0883, 'train_samples_per_second': 33.569, 'train_steps_per_second': 2.098, 'train_loss': 0.26288973450713854, 'epoch': 1.0}
Post-processing 1880518 example predictions split into 1893865 features.
{
    "68": {
        "QA": {
            "exact_match": 91.66104715572467,
            "f1": 96.74961387099688
        },
        "PR": {
            "p@1": 0.9605630749963875,
            "p@2": 0.9921848658542459,
            "p@3": 0.9961767255912528
        },
        "PRQA": {
            "exact_match": 88.79088194210298,
            "f1": 94.35669585631915
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1214122 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1214122 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
