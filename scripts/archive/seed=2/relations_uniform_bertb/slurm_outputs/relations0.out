2024-05-12 22:49:45 INFO     ------------- Experiment: model BERTbase, frequency threshold 0 ---------------
2024-05-12 22:49:45 INFO     Contexts were splited into 6517 paragraphs, which are 21.869127516778523 paragraphs on average per one report. The overall paragraph average length (characters) is 253.90072119073193
2024-05-12 22:49:46 INFO     Contexts were splited into 710 paragraphs, which are 16.904761904761905 paragraphs on average per one report. The overall paragraph average length (characters) is 253.5169014084507
2024-05-12 22:49:49 INFO     Contexts were splited into 1666 paragraphs, which are 19.372093023255815 paragraphs on average per one report. The overall paragraph average length (characters) is 253.16746698679472
2024-05-12 22:50:38 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-12 22:52:25 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-12 23:28:34 INFO     the model is trained
2024-05-13 00:08:26 INFO     evaluation data are prepared
2024-05-13 13:44:35 INFO     QA scores: {'exact_match': 91.7519628148933, 'f1': 97.55934763741557}
2024-05-13 13:44:36 INFO     PR scores: {'p@1': 0.9458058378690815, 'p@2': 0.9829728818457685, 'p@3': 0.9906495351861664}
2024-05-13 13:44:46 INFO     PRQA scores: {'exact_match': 87.78840132941573, 'f1': 93.94660288564695}
{'loss': 0.5989, 'grad_norm': 12.876797676086426, 'learning_rate': 2.6699669966996702e-05, 'epoch': 0.11001100110011001}
{'loss': 0.2662, 'grad_norm': 22.920955657958984, 'learning_rate': 2.33993399339934e-05, 'epoch': 0.22002200220022003}
{'loss': 0.2086, 'grad_norm': 67.91987609863281, 'learning_rate': 2.00990099009901e-05, 'epoch': 0.33003300330033003}
{'loss': 0.189, 'grad_norm': 21.417863845825195, 'learning_rate': 1.67986798679868e-05, 'epoch': 0.44004400440044006}
{'loss': 0.1481, 'grad_norm': 7.684791088104248, 'learning_rate': 1.3498349834983499e-05, 'epoch': 0.5500550055005501}
{'loss': 0.1213, 'grad_norm': 5.343430519104004, 'learning_rate': 1.0198019801980198e-05, 'epoch': 0.6600660066006601}
{'loss': 0.1144, 'grad_norm': 6.455575466156006, 'learning_rate': 6.897689768976898e-06, 'epoch': 0.77007700770077}
{'loss': 0.1036, 'grad_norm': 0.18430329859256744, 'learning_rate': 3.5973597359735973e-06, 'epoch': 0.8800880088008801}
{'loss': 0.0914, 'grad_norm': 5.344043731689453, 'learning_rate': 2.9702970297029703e-07, 'epoch': 0.9900990099009901}
{'eval_loss': 0.3791392743587494, 'eval_runtime': 953.4898, 'eval_samples_per_second': 180.637, 'eval_steps_per_second': 0.706, 'epoch': 1.0}
{'train_runtime': 2167.584, 'train_samples_per_second': 33.546, 'train_steps_per_second': 2.097, 'train_loss': 0.20336608703118084, 'epoch': 1.0}
Post-processing 4927331 example predictions split into 4927331 features.
{
    "0": {
        "QA": {
            "exact_match": 91.7519628148933,
            "f1": 97.55934763741557
        },
        "PR": {
            "p@1": 0.9458058378690815,
            "p@2": 0.9829728818457685,
            "p@3": 0.9906495351861664
        },
        "PRQA": {
            "exact_match": 87.78840132941573,
            "f1": 93.94660288564695
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1217176 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1217176 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
