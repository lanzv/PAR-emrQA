2024-05-12 06:05:40 INFO     ------------- Experiment: model BERTbase, frequency threshold 154 ---------------
2024-05-12 06:05:40 INFO     Contexts were splited into 716 paragraphs, which are 2.402684563758389 paragraphs on average per one report. The overall paragraph average length (characters) is 2310.9930167597763
2024-05-12 06:05:40 INFO     Contexts were splited into 79 paragraphs, which are 1.880952380952381 paragraphs on average per one report. The overall paragraph average length (characters) is 2278.4430379746836
2024-05-12 06:05:41 INFO     Contexts were splited into 188 paragraphs, which are 2.186046511627907 paragraphs on average per one report. The overall paragraph average length (characters) is 2243.494680851064
2024-05-12 06:06:08 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-12 06:12:01 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-12 07:46:21 INFO     the model is trained
2024-05-12 08:07:11 INFO     evaluation data are prepared
2024-05-12 12:21:31 INFO     QA scores: {'exact_match': 90.67060835219884, 'f1': 96.02369068623162}
2024-05-12 12:21:31 INFO     PR scores: {'p@1': 0.9784812870285632, 'p@2': 0.9958756803622176, 'p@3': 0.9985248783777275}
2024-05-12 12:21:41 INFO     PRQA scores: {'exact_match': 89.17742401618419, 'f1': 94.67384174952068}
{'loss': 0.6841, 'grad_norm': 15.667183876037598, 'learning_rate': 2.8743192291579387e-05, 'epoch': 0.04189359028068706}
{'loss': 0.3445, 'grad_norm': 17.819440841674805, 'learning_rate': 2.7486384583158777e-05, 'epoch': 0.08378718056137412}
{'loss': 0.2926, 'grad_norm': 19.287229537963867, 'learning_rate': 2.6229576874738163e-05, 'epoch': 0.12568077084206117}
{'loss': 0.2501, 'grad_norm': 0.9391357898712158, 'learning_rate': 2.4972769166317553e-05, 'epoch': 0.16757436112274823}
{'loss': 0.2311, 'grad_norm': 1.9355056285858154, 'learning_rate': 2.371596145789694e-05, 'epoch': 0.20946795140343527}
{'loss': 0.2152, 'grad_norm': 14.586600303649902, 'learning_rate': 2.2459153749476332e-05, 'epoch': 0.25136154168412234}
{'loss': 0.1877, 'grad_norm': 11.897035598754883, 'learning_rate': 2.120234604105572e-05, 'epoch': 0.2932551319648094}
{'loss': 0.1919, 'grad_norm': 16.583946228027344, 'learning_rate': 1.9945538332635108e-05, 'epoch': 0.33514872224549647}
{'loss': 0.1824, 'grad_norm': 5.668059825897217, 'learning_rate': 1.8688730624214495e-05, 'epoch': 0.3770423125261835}
{'loss': 0.161, 'grad_norm': 25.232452392578125, 'learning_rate': 1.7431922915793884e-05, 'epoch': 0.41893590280687054}
{'loss': 0.1549, 'grad_norm': 1.150356650352478, 'learning_rate': 1.617511520737327e-05, 'epoch': 0.4608294930875576}
{'loss': 0.1412, 'grad_norm': 0.06474753469228745, 'learning_rate': 1.491830749895266e-05, 'epoch': 0.5027230833682447}
{'loss': 0.1495, 'grad_norm': 0.47390440106391907, 'learning_rate': 1.3661499790532048e-05, 'epoch': 0.5446166736489317}
{'loss': 0.1203, 'grad_norm': 40.00995635986328, 'learning_rate': 1.2404692082111438e-05, 'epoch': 0.5865102639296188}
{'loss': 0.1143, 'grad_norm': 17.308456420898438, 'learning_rate': 1.1147884373690826e-05, 'epoch': 0.6284038542103059}
{'loss': 0.1171, 'grad_norm': 0.178047314286232, 'learning_rate': 9.891076665270214e-06, 'epoch': 0.6702974444909929}
{'loss': 0.1012, 'grad_norm': 0.1353519856929779, 'learning_rate': 8.634268956849602e-06, 'epoch': 0.7121910347716799}
{'loss': 0.0999, 'grad_norm': 0.048341501504182816, 'learning_rate': 7.377461248428991e-06, 'epoch': 0.754084625052367}
{'loss': 0.0977, 'grad_norm': 37.09852981567383, 'learning_rate': 6.120653540008379e-06, 'epoch': 0.795978215333054}
{'loss': 0.1026, 'grad_norm': 0.06168650835752487, 'learning_rate': 4.863845831587767e-06, 'epoch': 0.8378718056137411}
{'loss': 0.0949, 'grad_norm': 0.026223164051771164, 'learning_rate': 3.6070381231671554e-06, 'epoch': 0.8797653958944281}
{'loss': 0.0965, 'grad_norm': 11.904694557189941, 'learning_rate': 2.3502304147465435e-06, 'epoch': 0.9216589861751152}
{'loss': 0.0822, 'grad_norm': 0.09516120702028275, 'learning_rate': 1.0934227063259321e-06, 'epoch': 0.9635525764558023}
{'eval_loss': 0.3217543959617615, 'eval_runtime': 2444.2089, 'eval_samples_per_second': 179.44, 'eval_steps_per_second': 0.701, 'epoch': 1.0}
{'train_runtime': 5658.6361, 'train_samples_per_second': 33.745, 'train_steps_per_second': 2.109, 'train_loss': 0.17947137270904656, 'epoch': 1.0}
Post-processing 524311 example predictions split into 1485611 features.
{
    "154": {
        "QA": {
            "exact_match": 90.67060835219884,
            "f1": 96.02369068623162
        },
        "PR": {
            "p@1": 0.9784812870285632,
            "p@2": 0.9958756803622176,
            "p@3": 0.9985248783777275
        },
        "PRQA": {
            "exact_match": 89.17742401618419,
            "f1": 94.67384174952068
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 508878 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 508878 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
