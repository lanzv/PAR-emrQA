2024-05-11 16:07:13 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 134 ---------------
2024-05-11 16:07:13 INFO     Contexts were splited into 1556 paragraphs, which are 5.221476510067114 paragraphs on average per one report. The overall paragraph average length (characters) is 1063.4132390745501
2024-05-11 16:07:13 INFO     Contexts were splited into 171 paragraphs, which are 4.071428571428571 paragraphs on average per one report. The overall paragraph average length (characters) is 1052.6140350877192
2024-05-11 16:07:14 INFO     Contexts were splited into 397 paragraphs, which are 4.616279069767442 paragraphs on average per one report. The overall paragraph average length (characters) is 1062.410579345088
2024-05-11 16:07:45 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-11 16:10:44 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-11 16:48:13 INFO     the model is trained
2024-05-11 17:09:17 INFO     evaluation data are prepared
2024-05-11 20:51:22 INFO     QA scores: {'exact_match': 92.32274456914406, 'f1': 97.55188608630662}
2024-05-11 20:51:23 INFO     PR scores: {'p@1': 0.9831595298877703, 'p@2': 0.9985610038052117, 'p@3': 0.9994761813014787}
2024-05-11 20:51:33 INFO     PRQA scores: {'exact_match': 91.05654833582197, 'f1': 96.42889231346814}
{'loss': 0.7072, 'grad_norm': 12.960732460021973, 'learning_rate': 2.6799658630253898e-05, 'epoch': 0.10667804565820355}
{'loss': 0.2493, 'grad_norm': 8.999164581298828, 'learning_rate': 2.3599317260507788e-05, 'epoch': 0.2133560913164071}
{'loss': 0.1928, 'grad_norm': 2.2353432178497314, 'learning_rate': 2.0398975890761684e-05, 'epoch': 0.32003413697461064}
{'loss': 0.1604, 'grad_norm': 6.090859889984131, 'learning_rate': 1.7198634521015575e-05, 'epoch': 0.4267121826328142}
{'loss': 0.1475, 'grad_norm': 24.65476417541504, 'learning_rate': 1.3998293151269468e-05, 'epoch': 0.5333902282910177}
{'loss': 0.1153, 'grad_norm': 10.012484550476074, 'learning_rate': 1.0797951781523363e-05, 'epoch': 0.6400682739492213}
{'loss': 0.1071, 'grad_norm': 0.12075384706258774, 'learning_rate': 7.597610411777257e-06, 'epoch': 0.7467463196074248}
{'loss': 0.1035, 'grad_norm': 0.7198779582977295, 'learning_rate': 4.39726904203115e-06, 'epoch': 0.8534243652656284}
{'loss': 0.0918, 'grad_norm': 3.9393982887268066, 'learning_rate': 1.1969276722850437e-06, 'epoch': 0.9601024109238319}
{'eval_loss': 0.3559671938419342, 'eval_runtime': 990.2597, 'eval_samples_per_second': 179.946, 'eval_steps_per_second': 0.704, 'epoch': 1.0}
{'train_runtime': 2247.6416, 'train_samples_per_second': 33.363, 'train_steps_per_second': 2.085, 'train_loss': 0.20356824185920977, 'epoch': 1.0}
Post-processing 1164354 example predictions split into 1276345 features.
{
    "134": {
        "QA": {
            "exact_match": 92.32274456914406,
            "f1": 97.55188608630662
        },
        "PR": {
            "p@1": 0.9831595298877703,
            "p@2": 0.9985610038052117,
            "p@3": 0.9994761813014787
        },
        "PRQA": {
            "exact_match": 91.05654833582197,
            "f1": 96.42889231346814
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1213606 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1213606 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
