2024-05-11 15:55:06 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 68 ---------------
2024-05-11 15:55:06 INFO     Contexts were splited into 2489 paragraphs, which are 8.35234899328859 paragraphs on average per one report. The overall paragraph average length (characters) is 664.7934913619928
2024-05-11 15:55:07 INFO     Contexts were splited into 272 paragraphs, which are 6.476190476190476 paragraphs on average per one report. The overall paragraph average length (characters) is 661.7536764705883
2024-05-11 15:55:08 INFO     Contexts were splited into 637 paragraphs, which are 7.406976744186046 paragraphs on average per one report. The overall paragraph average length (characters) is 662.1302982731554
2024-05-11 15:55:37 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-11 15:57:58 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-11 16:33:23 INFO     the model is trained
2024-05-11 16:57:51 INFO     evaluation data are prepared
2024-05-11 22:13:36 INFO     QA scores: {'exact_match': 92.79117094552285, 'f1': 98.09415470832339}
2024-05-11 22:13:37 INFO     PR scores: {'p@1': 0.9813111121814941, 'p@2': 0.9965259380569337, 'p@3': 0.9989041953663118}
2024-05-11 22:13:47 INFO     PRQA scores: {'exact_match': 91.15709744231974, 'f1': 96.7159861330745}
{'loss': 0.6545, 'grad_norm': 11.17233657836914, 'learning_rate': 2.6653279785809908e-05, 'epoch': 0.11155734047300313}
{'loss': 0.2088, 'grad_norm': 9.712116241455078, 'learning_rate': 2.330655957161981e-05, 'epoch': 0.22311468094600626}
{'loss': 0.1692, 'grad_norm': 1.6964561939239502, 'learning_rate': 1.9959839357429718e-05, 'epoch': 0.33467202141900937}
{'loss': 0.1363, 'grad_norm': 23.545454025268555, 'learning_rate': 1.6613119143239628e-05, 'epoch': 0.4462293618920125}
{'loss': 0.1234, 'grad_norm': 15.509979248046875, 'learning_rate': 1.3266398929049533e-05, 'epoch': 0.5577867023650156}
{'loss': 0.1053, 'grad_norm': 15.708841323852539, 'learning_rate': 9.919678714859438e-06, 'epoch': 0.6693440428380187}
{'loss': 0.0884, 'grad_norm': 0.03528250381350517, 'learning_rate': 6.572958500669344e-06, 'epoch': 0.7809013833110219}
{'loss': 0.0947, 'grad_norm': 0.022648008540272713, 'learning_rate': 3.2262382864792503e-06, 'epoch': 0.892458723784025}
{'eval_loss': 0.3263668119907379, 'eval_runtime': 928.6904, 'eval_samples_per_second': 181.797, 'eval_steps_per_second': 0.711, 'epoch': 1.0}
{'train_runtime': 2124.4236, 'train_samples_per_second': 33.749, 'train_steps_per_second': 2.11, 'train_loss': 0.18450783044831234, 'epoch': 1.0}
Post-processing 1880518 example predictions split into 1887460 features.
{
    "68": {
        "QA": {
            "exact_match": 92.79117094552285,
            "f1": 98.09415470832339
        },
        "PR": {
            "p@1": 0.9813111121814941,
            "p@2": 0.9965259380569337,
            "p@3": 0.9989041953663118
        },
        "PRQA": {
            "exact_match": 91.15709744231974,
            "f1": 96.7159861330745
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1213516 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1213516 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
