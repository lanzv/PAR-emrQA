2024-05-11 12:22:56 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 142 ---------------
2024-05-11 12:22:56 INFO     Contexts were splited into 402 paragraphs, which are 2.19672131147541 paragraphs on average per one report. The overall paragraph average length (characters) is 2993.2213930348257
2024-05-11 12:22:56 INFO     Contexts were splited into 66 paragraphs, which are 2.5384615384615383 paragraphs on average per one report. The overall paragraph average length (characters) is 2744.151515151515
2024-05-11 12:22:57 INFO     Contexts were splited into 113 paragraphs, which are 2.1320754716981134 paragraphs on average per one report. The overall paragraph average length (characters) is 2999.8407079646017
2024-05-11 12:23:04 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-11 12:26:31 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-11 13:37:27 INFO     the model is trained
2024-05-11 13:42:49 INFO     evaluation data are prepared
2024-05-11 14:48:46 INFO     QA scores: {'exact_match': 30.5656973497702, 'f1': 72.42378552158092}
2024-05-11 14:48:46 INFO     PR scores: {'p@1': 0.9535458098878914, 'p@2': 0.9981315235599846, 'p@3': 0.9998281860744813}
2024-05-11 14:48:49 INFO     PRQA scores: {'exact_match': 30.08891370645591, 'f1': 70.78334026155021}
{'loss': 1.1104, 'grad_norm': 6.867945671081543, 'learning_rate': 2.886018237082067e-05, 'epoch': 0.037993920972644375}
{'loss': 0.7147, 'grad_norm': 3.2516496181488037, 'learning_rate': 2.7720364741641338e-05, 'epoch': 0.07598784194528875}
{'loss': 0.6345, 'grad_norm': 8.688055038452148, 'learning_rate': 2.6580547112462007e-05, 'epoch': 0.11398176291793313}
{'loss': 0.5596, 'grad_norm': 11.286219596862793, 'learning_rate': 2.5440729483282676e-05, 'epoch': 0.1519756838905775}
{'loss': 0.5348, 'grad_norm': 8.609375, 'learning_rate': 2.4300911854103345e-05, 'epoch': 0.1899696048632219}
{'loss': 0.4797, 'grad_norm': 2.9745969772338867, 'learning_rate': 2.3161094224924013e-05, 'epoch': 0.22796352583586627}
{'loss': 0.4393, 'grad_norm': 7.518169403076172, 'learning_rate': 2.2021276595744682e-05, 'epoch': 0.26595744680851063}
{'loss': 0.3821, 'grad_norm': 3.0551302433013916, 'learning_rate': 2.0881458966565347e-05, 'epoch': 0.303951367781155}
{'loss': 0.3892, 'grad_norm': 16.735429763793945, 'learning_rate': 1.974164133738602e-05, 'epoch': 0.34194528875379937}
{'loss': 0.352, 'grad_norm': 5.048095226287842, 'learning_rate': 1.8601823708206688e-05, 'epoch': 0.3799392097264438}
{'loss': 0.3355, 'grad_norm': 9.759397506713867, 'learning_rate': 1.7462006079027357e-05, 'epoch': 0.41793313069908816}
{'loss': 0.3099, 'grad_norm': 2.1388487815856934, 'learning_rate': 1.6322188449848026e-05, 'epoch': 0.45592705167173253}
{'loss': 0.2841, 'grad_norm': 5.71746301651001, 'learning_rate': 1.5182370820668691e-05, 'epoch': 0.4939209726443769}
{'loss': 0.2831, 'grad_norm': 19.271181106567383, 'learning_rate': 1.4042553191489362e-05, 'epoch': 0.5319148936170213}
{'loss': 0.2565, 'grad_norm': 7.850693225860596, 'learning_rate': 1.2902735562310032e-05, 'epoch': 0.5699088145896657}
{'loss': 0.2574, 'grad_norm': 5.327973365783691, 'learning_rate': 1.17629179331307e-05, 'epoch': 0.60790273556231}
{'loss': 0.2218, 'grad_norm': 12.314276695251465, 'learning_rate': 1.0623100303951368e-05, 'epoch': 0.6458966565349544}
{'loss': 0.2152, 'grad_norm': 4.00985050201416, 'learning_rate': 9.483282674772037e-06, 'epoch': 0.6838905775075987}
{'loss': 0.2048, 'grad_norm': 10.353270530700684, 'learning_rate': 8.343465045592705e-06, 'epoch': 0.7218844984802432}
{'loss': 0.1891, 'grad_norm': 1.0534969568252563, 'learning_rate': 7.203647416413374e-06, 'epoch': 0.7598784194528876}
{'loss': 0.1816, 'grad_norm': 14.010601043701172, 'learning_rate': 6.063829787234042e-06, 'epoch': 0.7978723404255319}
{'loss': 0.176, 'grad_norm': 6.084945201873779, 'learning_rate': 4.924012158054712e-06, 'epoch': 0.8358662613981763}
{'loss': 0.1665, 'grad_norm': 1.5528696775436401, 'learning_rate': 3.7841945288753804e-06, 'epoch': 0.8738601823708206}
{'loss': 0.1664, 'grad_norm': 7.779642581939697, 'learning_rate': 2.6443768996960487e-06, 'epoch': 0.9118541033434651}
{'loss': 0.1558, 'grad_norm': 6.386005401611328, 'learning_rate': 1.5045592705167174e-06, 'epoch': 0.9498480243161094}
{'loss': 0.1647, 'grad_norm': 9.469561576843262, 'learning_rate': 3.6474164133738603e-07, 'epoch': 0.9878419452887538}
{'eval_loss': 1.0834448337554932, 'eval_runtime': 709.3655, 'eval_samples_per_second': 179.858, 'eval_steps_per_second': 0.703, 'epoch': 1.0}
{'train_runtime': 4255.102, 'train_samples_per_second': 49.481, 'train_steps_per_second': 3.093, 'train_loss': 0.34999986419561785, 'epoch': 1.0}
Post-processing 110915 example predictions split into 390292 features.
{
    "142": {
        "QA": {
            "exact_match": 30.5656973497702,
            "f1": 72.42378552158092
        },
        "PR": {
            "p@1": 0.9535458098878914,
            "p@2": 0.9981315235599846,
            "p@3": 0.9998281860744813
        },
        "PRQA": {
            "exact_match": 30.08891370645591,
            "f1": 70.78334026155021
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 713201 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 713201 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
