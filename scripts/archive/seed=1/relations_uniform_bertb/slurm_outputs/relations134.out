2024-05-09 23:07:27 INFO     ------------- Experiment: model BERTbase, frequency threshold 134 ---------------
2024-05-09 23:07:27 INFO     Contexts were splited into 1556 paragraphs, which are 5.221476510067114 paragraphs on average per one report. The overall paragraph average length (characters) is 1063.4132390745501
2024-05-09 23:07:27 INFO     Contexts were splited into 171 paragraphs, which are 4.071428571428571 paragraphs on average per one report. The overall paragraph average length (characters) is 1052.6140350877192
2024-05-09 23:07:28 INFO     Contexts were splited into 397 paragraphs, which are 4.616279069767442 paragraphs on average per one report. The overall paragraph average length (characters) is 1062.410579345088
2024-05-09 23:07:56 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-09 23:10:52 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-09 23:51:28 INFO     the model is trained
2024-05-10 00:11:39 INFO     evaluation data are prepared
2024-05-10 04:08:18 INFO     QA scores: {'exact_match': 90.03841337122489, 'f1': 96.09700167729233}
2024-05-10 04:08:18 INFO     PR scores: {'p@1': 0.9657350320312124, 'p@2': 0.9930518761138674, 'p@3': 0.9969233177592601}
2024-05-10 04:08:28 INFO     PRQA scores: {'exact_match': 87.73361109773133, 'f1': 93.95188817140914}
{'loss': 0.8777, 'grad_norm': 8.632010459899902, 'learning_rate': 2.7020262216924913e-05, 'epoch': 0.09932459276916965}
{'loss': 0.3949, 'grad_norm': 2.1410677433013916, 'learning_rate': 2.4040524433849822e-05, 'epoch': 0.1986491855383393}
{'loss': 0.3347, 'grad_norm': 11.338500022888184, 'learning_rate': 2.1060786650774734e-05, 'epoch': 0.29797377830750893}
{'loss': 0.2627, 'grad_norm': 19.250423431396484, 'learning_rate': 1.8081048867699643e-05, 'epoch': 0.3972983710766786}
{'loss': 0.2343, 'grad_norm': 9.524677276611328, 'learning_rate': 1.5101311084624554e-05, 'epoch': 0.49662296384584825}
{'loss': 0.1941, 'grad_norm': 10.212682723999023, 'learning_rate': 1.2121573301549464e-05, 'epoch': 0.5959475566150179}
{'loss': 0.1674, 'grad_norm': 4.179891586303711, 'learning_rate': 9.141835518474375e-06, 'epoch': 0.6952721493841876}
{'loss': 0.1554, 'grad_norm': 5.868069171905518, 'learning_rate': 6.162097735399284e-06, 'epoch': 0.7945967421533572}
{'loss': 0.1569, 'grad_norm': 3.046680212020874, 'learning_rate': 3.1823599523241957e-06, 'epoch': 0.8939213349225268}
{'loss': 0.1279, 'grad_norm': 3.3357231616973877, 'learning_rate': 2.026221692491061e-07, 'epoch': 0.9932459276916965}
{'eval_loss': 0.4764896035194397, 'eval_runtime': 1088.9587, 'eval_samples_per_second': 181.354, 'eval_steps_per_second': 0.709, 'epoch': 1.0}
{'train_runtime': 2435.2647, 'train_samples_per_second': 33.071, 'train_steps_per_second': 2.067, 'train_loss': 0.28953215694162265, 'epoch': 1.0}
Post-processing 1164354 example predictions split into 1360975 features.
{
    "134": {
        "QA": {
            "exact_match": 90.03841337122489,
            "f1": 96.09700167729233
        },
        "PR": {
            "p@1": 0.9657350320312124,
            "p@2": 0.9930518761138674,
            "p@3": 0.9969233177592601
        },
        "PRQA": {
            "exact_match": 87.73361109773133,
            "f1": 93.95188817140914
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1207993 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1207993 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
