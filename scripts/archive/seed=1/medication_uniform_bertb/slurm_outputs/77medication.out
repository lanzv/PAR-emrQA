2024-05-09 15:16:44 INFO     ------------- Experiment: model BERTbase, frequency threshold 77 ---------------
2024-05-09 15:16:44 INFO     Contexts were splited into 1288 paragraphs, which are 7.038251366120218 paragraphs on average per one report. The overall paragraph average length (characters) is 934.2197204968944
2024-05-09 15:16:45 INFO     Contexts were splited into 194 paragraphs, which are 7.461538461538462 paragraphs on average per one report. The overall paragraph average length (characters) is 933.5773195876288
2024-05-09 15:16:45 INFO     Contexts were splited into 361 paragraphs, which are 6.811320754716981 paragraphs on average per one report. The overall paragraph average length (characters) is 939.0083102493074
2024-05-09 15:16:52 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-09 15:18:18 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-09 15:44:52 INFO     the model is trained
2024-05-09 15:50:09 INFO     evaluation data are prepared
2024-05-09 17:01:40 INFO     QA scores: {'exact_match': 28.74446973927237, 'f1': 71.26997994420944}
2024-05-09 17:01:40 INFO     PR scores: {'p@1': 0.8627851037326575, 'p@2': 0.9726386323611529, 'p@3': 0.9887032343971479}
2024-05-09 17:01:43 INFO     PRQA scores: {'exact_match': 27.215325802156265, 'f1': 65.87635039693417}
{'loss': 1.9565, 'grad_norm': 14.31065845489502, 'learning_rate': 2.6979460330245673e-05, 'epoch': 0.10068465565847765}
{'loss': 1.2545, 'grad_norm': 14.718284606933594, 'learning_rate': 2.395892066049134e-05, 'epoch': 0.2013693113169553}
{'loss': 1.0079, 'grad_norm': 13.908453941345215, 'learning_rate': 2.093838099073701e-05, 'epoch': 0.30205396697543296}
{'loss': 0.8472, 'grad_norm': 8.883752822875977, 'learning_rate': 1.7917841320982683e-05, 'epoch': 0.4027386226339106}
{'loss': 0.7137, 'grad_norm': 13.279532432556152, 'learning_rate': 1.4897301651228352e-05, 'epoch': 0.5034232782923882}
{'loss': 0.6099, 'grad_norm': 16.87555503845215, 'learning_rate': 1.1876761981474025e-05, 'epoch': 0.6041079339508659}
{'loss': 0.5447, 'grad_norm': 18.672563552856445, 'learning_rate': 8.856222311719694e-06, 'epoch': 0.7047925896093435}
{'loss': 0.4831, 'grad_norm': 7.82075309753418, 'learning_rate': 5.835682641965364e-06, 'epoch': 0.8054772452678212}
{'loss': 0.448, 'grad_norm': 15.648183822631836, 'learning_rate': 2.815142972211035e-06, 'epoch': 0.9061619009262988}
{'eval_loss': 1.8170928955078125, 'eval_runtime': 262.3266, 'eval_samples_per_second': 179.81, 'eval_steps_per_second': 0.705, 'epoch': 1.0}
{'train_runtime': 1594.1909, 'train_samples_per_second': 49.833, 'train_steps_per_second': 3.115, 'train_loss': 0.832152764162516, 'epoch': 1.0}
Post-processing 354547 example predictions split into 415153 features.
{
    "77": {
        "QA": {
            "exact_match": 28.74446973927237,
            "f1": 71.26997994420944
        },
        "PR": {
            "p@1": 0.8627851037326575,
            "p@2": 0.9726386323611529,
            "p@3": 0.9887032343971479
        },
        "PRQA": {
            "exact_match": 27.215325802156265,
            "f1": 65.87635039693417
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1206977 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1206977 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
