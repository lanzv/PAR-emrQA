2024-05-12 22:11:37 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 0 ---------------
2024-05-12 22:11:37 INFO     Contexts were splited into 3546 paragraphs, which are 19.37704918032787 paragraphs on average per one report. The overall paragraph average length (characters) is 339.3330513254371
2024-05-12 22:11:38 INFO     Contexts were splited into 532 paragraphs, which are 20.46153846153846 paragraphs on average per one report. The overall paragraph average length (characters) is 340.4398496240602
2024-05-12 22:11:38 INFO     Contexts were splited into 1002 paragraphs, which are 18.90566037735849 paragraphs on average per one report. The overall paragraph average length (characters) is 338.3053892215569
2024-05-12 22:11:49 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-12 22:12:41 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-12 22:35:09 INFO     the model is trained
2024-05-12 22:43:59 INFO     evaluation data are prepared
2024-05-13 01:22:00 INFO     QA scores: {'exact_match': 33.38344572827628, 'f1': 77.07735059890999}
2024-05-13 01:22:01 INFO     PR scores: {'p@1': 0.8201322967226494, 'p@2': 0.9493793221940638, 'p@3': 0.9824535028564065}
2024-05-13 01:22:03 INFO     PRQA scores: {'exact_match': 30.269318328250506, 'f1': 68.78600559746472}
{'loss': 1.4927, 'grad_norm': 10.690164566040039, 'learning_rate': 2.6382927417410177e-05, 'epoch': 0.12056908608632746}
{'loss': 0.972, 'grad_norm': 8.808648109436035, 'learning_rate': 2.276585483482035e-05, 'epoch': 0.24113817217265493}
{'loss': 0.7734, 'grad_norm': 10.832840919494629, 'learning_rate': 1.9148782252230528e-05, 'epoch': 0.3617072582589824}
{'loss': 0.6217, 'grad_norm': 12.262360572814941, 'learning_rate': 1.5531709669640704e-05, 'epoch': 0.48227634434530986}
{'loss': 0.514, 'grad_norm': 6.577267646789551, 'learning_rate': 1.191463708705088e-05, 'epoch': 0.6028454304316373}
{'loss': 0.4368, 'grad_norm': 13.529411315917969, 'learning_rate': 8.297564504461056e-06, 'epoch': 0.7234145165179648}
{'loss': 0.3782, 'grad_norm': 12.592845916748047, 'learning_rate': 4.680491921871233e-06, 'epoch': 0.8439836026042923}
{'loss': 0.3389, 'grad_norm': 14.41087532043457, 'learning_rate': 1.0634193392814083e-06, 'epoch': 0.9645526886906197}
{'eval_loss': 1.6193689107894897, 'eval_runtime': 238.3726, 'eval_samples_per_second': 180.113, 'eval_steps_per_second': 0.705, 'epoch': 1.0}
{'train_runtime': 1347.271, 'train_samples_per_second': 49.239, 'train_steps_per_second': 3.078, 'train_loss': 0.678471840517418, 'epoch': 1.0}
Post-processing 982965 example predictions split into 982965 features.
{
    "0": {
        "QA": {
            "exact_match": 33.38344572827628,
            "f1": 77.07735059890999
        },
        "PR": {
            "p@1": 0.8201322967226494,
            "p@2": 0.9493793221940638,
            "p@3": 0.9824535028564065
        },
        "PRQA": {
            "exact_match": 30.269318328250506,
            "f1": 68.78600559746472
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 2755172 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 2755172 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
