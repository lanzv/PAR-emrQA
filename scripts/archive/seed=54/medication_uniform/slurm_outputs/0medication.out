2024-05-08 11:34:47 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 0 ---------------
2024-05-08 11:34:48 INFO     Contexts were splited into 3546 paragraphs, which are 19.37704918032787 paragraphs on average per one report. The overall paragraph average length (characters) is 339.3330513254371
2024-05-08 11:34:48 INFO     Contexts were splited into 532 paragraphs, which are 20.46153846153846 paragraphs on average per one report. The overall paragraph average length (characters) is 340.4398496240602
2024-05-08 11:34:49 INFO     Contexts were splited into 1002 paragraphs, which are 18.90566037735849 paragraphs on average per one report. The overall paragraph average length (characters) is 338.3053892215569
2024-05-08 11:35:00 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-08 11:35:51 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-08 11:58:18 INFO     the model is trained
2024-05-08 12:07:12 INFO     evaluation data are prepared
2024-05-08 14:45:02 INFO     QA scores: {'exact_match': 33.961170052832784, 'f1': 77.6163300278698}
2024-05-08 14:45:02 INFO     PR scores: {'p@1': 0.8182852970233238, 'p@2': 0.9483913921223315, 'p@3': 0.9776641896825737}
2024-05-08 14:45:05 INFO     PRQA scores: {'exact_match': 30.68381942356428, 'f1': 69.10951675015261}
{'loss': 1.4909, 'grad_norm': 10.208261489868164, 'learning_rate': 2.639769452449568e-05, 'epoch': 0.12007684918347743}
{'loss': 0.9578, 'grad_norm': 13.116710662841797, 'learning_rate': 2.2795389048991354e-05, 'epoch': 0.24015369836695485}
{'loss': 0.7516, 'grad_norm': 13.521984100341797, 'learning_rate': 1.9193083573487033e-05, 'epoch': 0.36023054755043227}
{'loss': 0.6274, 'grad_norm': 16.539325714111328, 'learning_rate': 1.559077809798271e-05, 'epoch': 0.4803073967339097}
{'loss': 0.5071, 'grad_norm': 18.16200828552246, 'learning_rate': 1.1988472622478386e-05, 'epoch': 0.6003842459173871}
{'loss': 0.4301, 'grad_norm': 16.318443298339844, 'learning_rate': 8.386167146974064e-06, 'epoch': 0.7204610951008645}
{'loss': 0.3835, 'grad_norm': 9.781596183776855, 'learning_rate': 4.783861671469741e-06, 'epoch': 0.840537944284342}
{'loss': 0.3427, 'grad_norm': 13.660079956054688, 'learning_rate': 1.181556195965418e-06, 'epoch': 0.9606147934678194}
{'eval_loss': 1.6256438493728638, 'eval_runtime': 237.6501, 'eval_samples_per_second': 180.661, 'eval_steps_per_second': 0.707, 'epoch': 1.0}
{'train_runtime': 1345.748, 'train_samples_per_second': 49.504, 'train_steps_per_second': 3.094, 'train_loss': 0.6726871155180917, 'epoch': 1.0}
Post-processing 982965 example predictions split into 982965 features.
{
    "0": {
        "QA": {
            "exact_match": 33.961170052832784,
            "f1": 77.6163300278698
        },
        "PR": {
            "p@1": 0.8182852970233238,
            "p@2": 0.9483913921223315,
            "p@3": 0.9776641896825737
        },
        "PRQA": {
            "exact_match": 30.68381942356428,
            "f1": 69.10951675015261
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 634506 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 634506 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
