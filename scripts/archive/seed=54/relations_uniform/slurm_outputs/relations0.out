2024-05-08 11:33:30 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 0 ---------------
2024-05-08 11:33:30 INFO     Contexts were splited into 6517 paragraphs, which are 21.869127516778523 paragraphs on average per one report. The overall paragraph average length (characters) is 253.90072119073193
2024-05-08 11:33:31 INFO     Contexts were splited into 710 paragraphs, which are 16.904761904761905 paragraphs on average per one report. The overall paragraph average length (characters) is 253.5169014084507
2024-05-08 11:33:33 INFO     Contexts were splited into 1666 paragraphs, which are 19.372093023255815 paragraphs on average per one report. The overall paragraph average length (characters) is 253.16746698679472
2024-05-08 11:34:18 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-08 11:35:59 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-08 12:12:20 INFO     the model is trained
2024-05-08 12:52:50 INFO     evaluation data are prepared
2024-05-09 02:26:36 INFO     QA scores: {'exact_match': 93.30595828717307, 'f1': 98.3376079884703}
2024-05-09 02:26:36 INFO     PR scores: {'p@1': 0.9691308222147296, 'p@2': 0.9947437503010452, 'p@3': 0.9976699099272675}
2024-05-09 02:26:46 INFO     PRQA scores: {'exact_match': 90.65615818120514, 'f1': 96.09908805663211}
{'loss': 0.5236, 'grad_norm': 9.972503662109375, 'learning_rate': 2.6706915477497258e-05, 'epoch': 0.10976948408342481}
{'loss': 0.1559, 'grad_norm': 6.306290626525879, 'learning_rate': 2.3413830954994515e-05, 'epoch': 0.21953896816684962}
{'loss': 0.1261, 'grad_norm': 4.459084510803223, 'learning_rate': 2.0120746432491768e-05, 'epoch': 0.32930845225027444}
{'loss': 0.1028, 'grad_norm': 3.7466084957122803, 'learning_rate': 1.6827661909989025e-05, 'epoch': 0.43907793633369924}
{'loss': 0.0858, 'grad_norm': 10.1140775680542, 'learning_rate': 1.353457738748628e-05, 'epoch': 0.5488474204171241}
{'loss': 0.0631, 'grad_norm': 0.11031179130077362, 'learning_rate': 1.0241492864983535e-05, 'epoch': 0.6586169045005489}
{'loss': 0.0588, 'grad_norm': 2.742889404296875, 'learning_rate': 6.9484083424807905e-06, 'epoch': 0.7683863885839737}
{'loss': 0.0643, 'grad_norm': 7.924244403839111, 'learning_rate': 3.655323819978046e-06, 'epoch': 0.8781558726673985}
{'loss': 0.0634, 'grad_norm': 0.08255533874034882, 'learning_rate': 3.622392974753019e-07, 'epoch': 0.9879253567508233}
{'eval_loss': 0.2831724286079407, 'eval_runtime': 960.7523, 'eval_samples_per_second': 179.272, 'eval_steps_per_second': 0.7, 'epoch': 1.0}
{'train_runtime': 2180.053, 'train_samples_per_second': 33.425, 'train_steps_per_second': 2.089, 'train_loss': 0.13690499817108087, 'epoch': 1.0}
Post-processing 4927331 example predictions split into 4927331 features.
{
    "0": {
        "QA": {
            "exact_match": 93.30595828717307,
            "f1": 98.3376079884703
        },
        "PR": {
            "p@1": 0.9691308222147296,
            "p@2": 0.9947437503010452,
            "p@3": 0.9976699099272675
        },
        "PRQA": {
            "exact_match": 90.65615818120514,
            "f1": 96.09908805663211
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 483487 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 483487 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
