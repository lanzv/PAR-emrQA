2024-05-07 06:03:02 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 134 ---------------
2024-05-07 06:03:02 INFO     Contexts were splited into 1556 paragraphs, which are 5.221476510067114 paragraphs on average per one report. The overall paragraph average length (characters) is 1063.4132390745501
2024-05-07 06:03:02 INFO     Contexts were splited into 171 paragraphs, which are 4.071428571428571 paragraphs on average per one report. The overall paragraph average length (characters) is 1052.6140350877192
2024-05-07 06:03:04 INFO     Contexts were splited into 397 paragraphs, which are 4.616279069767442 paragraphs on average per one report. The overall paragraph average length (characters) is 1062.410579345088
2024-05-07 06:03:33 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-07 06:06:27 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-07 06:43:50 INFO     the model is trained
2024-05-07 07:04:21 INFO     evaluation data are prepared
2024-05-07 10:46:08 INFO     QA scores: {'exact_match': 91.67188478396994, 'f1': 97.36514756986233}
2024-05-07 10:46:09 INFO     PR scores: {'p@1': 0.9828223592312509, 'p@2': 0.9974591782669429, 'p@3': 0.9990366552670873}
2024-05-07 10:46:19 INFO     PRQA scores: {'exact_match': 90.37498193728626, 'f1': 96.19623333955107}
{'loss': 0.7202, 'grad_norm': 11.362957000732422, 'learning_rate': 2.6809868141216504e-05, 'epoch': 0.10633772862611654}
{'loss': 0.2292, 'grad_norm': 2.0835912227630615, 'learning_rate': 2.3619736282433008e-05, 'epoch': 0.21267545725223308}
{'loss': 0.1906, 'grad_norm': 14.305434226989746, 'learning_rate': 2.042960442364951e-05, 'epoch': 0.31901318587834965}
{'loss': 0.1639, 'grad_norm': 1.343575358390808, 'learning_rate': 1.7239472564866015e-05, 'epoch': 0.42535091450446616}
{'loss': 0.1388, 'grad_norm': 17.905216217041016, 'learning_rate': 1.4049340706082518e-05, 'epoch': 0.5316886431305827}
{'loss': 0.1235, 'grad_norm': 0.07758186757564545, 'learning_rate': 1.0859208847299022e-05, 'epoch': 0.6380263717566993}
{'loss': 0.1063, 'grad_norm': 2.486942768096924, 'learning_rate': 7.669076988515525e-06, 'epoch': 0.7443641003828159}
{'loss': 0.0871, 'grad_norm': 0.19133618474006653, 'learning_rate': 4.478945129732029e-06, 'epoch': 0.8507018290089323}
{'loss': 0.0926, 'grad_norm': 0.16164058446884155, 'learning_rate': 1.2888132709485326e-06, 'epoch': 0.9570395576350489}
{'eval_loss': 0.3572064936161041, 'eval_runtime': 984.348, 'eval_samples_per_second': 181.026, 'eval_steps_per_second': 0.708, 'epoch': 1.0}
{'train_runtime': 2242.4293, 'train_samples_per_second': 33.546, 'train_steps_per_second': 2.097, 'train_loss': 0.20016719675327757, 'epoch': 1.0}
Post-processing 1164354 example predictions split into 1276345 features.
{
    "134": {
        "QA": {
            "exact_match": 91.67188478396994,
            "f1": 97.36514756986233
        },
        "PR": {
            "p@1": 0.9828223592312509,
            "p@2": 0.9974591782669429,
            "p@3": 0.9990366552670873
        },
        "PRQA": {
            "exact_match": 90.37498193728626,
            "f1": 96.19623333955107
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1198350 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1198350 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
