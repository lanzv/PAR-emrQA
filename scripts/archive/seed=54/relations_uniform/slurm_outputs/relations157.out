2024-05-06 23:44:41 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 157 ---------------
2024-05-06 23:44:41 INFO     Contexts were splited into 587 paragraphs, which are 1.9697986577181208 paragraphs on average per one report. The overall paragraph average length (characters) is 2818.8603066439523
2024-05-06 23:44:41 INFO     Contexts were splited into 67 paragraphs, which are 1.5952380952380953 paragraphs on average per one report. The overall paragraph average length (characters) is 2686.5223880597014
2024-05-06 23:44:42 INFO     Contexts were splited into 151 paragraphs, which are 1.755813953488372 paragraphs on average per one report. The overall paragraph average length (characters) is 2793.225165562914
2024-05-06 23:45:12 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-06 23:52:32 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-07 01:36:21 INFO     the model is trained
2024-05-07 01:57:38 INFO     evaluation data are prepared
2024-05-07 05:56:55 INFO     QA scores: {'exact_match': 91.76882134771928, 'f1': 97.34218234393599}
2024-05-07 05:56:55 INFO     PR scores: {'p@1': 0.988602427628727, 'p@2': 0.9985489619960503, 'p@3': 0.9998856028129666}
2024-05-07 05:57:05 INFO     PRQA scores: {'exact_match': 90.88796300756226, 'f1': 96.61505875021719}
{'loss': 0.6108, 'grad_norm': 2.8177809715270996, 'learning_rate': 2.8880764065064915e-05, 'epoch': 0.03730786449783614}
{'loss': 0.2107, 'grad_norm': 0.5678039193153381, 'learning_rate': 2.7761528130129833e-05, 'epoch': 0.07461572899567229}
{'loss': 0.18, 'grad_norm': 1.0492554903030396, 'learning_rate': 2.6642292195194747e-05, 'epoch': 0.11192359349350843}
{'loss': 0.1605, 'grad_norm': 2.0057289600372314, 'learning_rate': 2.552305626025966e-05, 'epoch': 0.14923145799134457}
{'loss': 0.1372, 'grad_norm': 13.25424575805664, 'learning_rate': 2.440382032532458e-05, 'epoch': 0.18653932248918073}
{'loss': 0.1393, 'grad_norm': 0.11602480709552765, 'learning_rate': 2.3284584390389493e-05, 'epoch': 0.22384718698701686}
{'loss': 0.1321, 'grad_norm': 2.81353497505188, 'learning_rate': 2.216534845545441e-05, 'epoch': 0.261155051484853}
{'loss': 0.1152, 'grad_norm': 5.2578935623168945, 'learning_rate': 2.1046112520519325e-05, 'epoch': 0.29846291598268915}
{'loss': 0.1031, 'grad_norm': 0.6120854020118713, 'learning_rate': 1.992687658558424e-05, 'epoch': 0.3357707804805253}
{'loss': 0.1051, 'grad_norm': 3.487717390060425, 'learning_rate': 1.8807640650649157e-05, 'epoch': 0.37307864497836146}
{'loss': 0.0995, 'grad_norm': 0.35887646675109863, 'learning_rate': 1.7688404715714074e-05, 'epoch': 0.4103865094761976}
{'loss': 0.0902, 'grad_norm': 0.23740136623382568, 'learning_rate': 1.656916878077899e-05, 'epoch': 0.4476943739740337}
{'loss': 0.0873, 'grad_norm': 0.05447932705283165, 'learning_rate': 1.5449932845843906e-05, 'epoch': 0.48500223847186985}
{'loss': 0.0795, 'grad_norm': 13.09332275390625, 'learning_rate': 1.4330696910908819e-05, 'epoch': 0.522310102969706}
{'loss': 0.069, 'grad_norm': 0.035935595631599426, 'learning_rate': 1.3211460975973735e-05, 'epoch': 0.5596179674675421}
{'loss': 0.0773, 'grad_norm': 0.4762158691883087, 'learning_rate': 1.209222504103865e-05, 'epoch': 0.5969258319653783}
{'loss': 0.0747, 'grad_norm': 0.039582185447216034, 'learning_rate': 1.0972989106103568e-05, 'epoch': 0.6342336964632145}
{'loss': 0.0569, 'grad_norm': 23.474546432495117, 'learning_rate': 9.853753171168483e-06, 'epoch': 0.6715415609610506}
{'loss': 0.0649, 'grad_norm': 7.783709526062012, 'learning_rate': 8.734517236233398e-06, 'epoch': 0.7088494254588867}
{'loss': 0.0573, 'grad_norm': 0.41120684146881104, 'learning_rate': 7.615281301298314e-06, 'epoch': 0.7461572899567229}
{'loss': 0.0514, 'grad_norm': 0.48247504234313965, 'learning_rate': 6.4960453663632295e-06, 'epoch': 0.783465154454559}
{'loss': 0.0637, 'grad_norm': 27.71567726135254, 'learning_rate': 5.3768094314281454e-06, 'epoch': 0.8207730189523952}
{'loss': 0.0401, 'grad_norm': 0.1446012705564499, 'learning_rate': 4.2575734964930605e-06, 'epoch': 0.8580808834502313}
{'loss': 0.0538, 'grad_norm': 27.44478416442871, 'learning_rate': 3.1383375615579765e-06, 'epoch': 0.8953887479480674}
{'loss': 0.0469, 'grad_norm': 0.03299211338162422, 'learning_rate': 2.019101626622892e-06, 'epoch': 0.9326966124459036}
{'loss': 0.0511, 'grad_norm': 15.625185012817383, 'learning_rate': 8.998656916878078e-07, 'epoch': 0.9700044769437397}
{'eval_loss': 0.2513759434223175, 'eval_runtime': 2637.5285, 'eval_samples_per_second': 180.137, 'eval_steps_per_second': 0.704, 'epoch': 1.0}
{'train_runtime': 6228.4478, 'train_samples_per_second': 34.426, 'train_steps_per_second': 2.152, 'train_loss': 0.11161623469325539, 'epoch': 1.0}
Post-processing 402318 example predictions split into 1406525 features.
{
    "157": {
        "QA": {
            "exact_match": 91.76882134771928,
            "f1": 97.34218234393599
        },
        "PR": {
            "p@1": 0.988602427628727,
            "p@2": 0.9985489619960503,
            "p@3": 0.9998856028129666
        },
        "PRQA": {
            "exact_match": 90.88796300756226,
            "f1": 96.61505875021719
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 628790 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 628790 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
