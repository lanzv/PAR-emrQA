2024-05-07 18:47:29 INFO     ------------- Experiment: model BERTbase, frequency threshold 68 ---------------
2024-05-07 18:47:29 INFO     Contexts were splited into 2489 paragraphs, which are 8.35234899328859 paragraphs on average per one report. The overall paragraph average length (characters) is 664.7934913619928
2024-05-07 18:47:29 INFO     Contexts were splited into 272 paragraphs, which are 6.476190476190476 paragraphs on average per one report. The overall paragraph average length (characters) is 661.7536764705883
2024-05-07 18:47:31 INFO     Contexts were splited into 637 paragraphs, which are 7.406976744186046 paragraphs on average per one report. The overall paragraph average length (characters) is 662.1302982731554
2024-05-07 18:48:01 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-07 18:50:22 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-07 19:25:54 INFO     the model is trained
2024-05-07 19:49:38 INFO     evaluation data are prepared
2024-05-08 01:07:44 INFO     QA scores: {'exact_match': 90.88435046481383, 'f1': 96.58216385349634}
2024-05-08 01:07:44 INFO     PR scores: {'p@1': 0.963766196233322, 'p@2': 0.9919982178122441, 'p@3': 0.9962008092095757}
2024-05-08 01:07:54 INFO     PRQA scores: {'exact_match': 88.60844853330764, 'f1': 94.45309752461011}
{'loss': 0.7434, 'grad_norm': 11.220258712768555, 'learning_rate': 2.6663701067615658e-05, 'epoch': 0.11120996441281139}
{'loss': 0.339, 'grad_norm': 12.258301734924316, 'learning_rate': 2.3327402135231316e-05, 'epoch': 0.22241992882562278}
{'loss': 0.2802, 'grad_norm': 7.873308181762695, 'learning_rate': 1.9991103202846976e-05, 'epoch': 0.33362989323843417}
{'loss': 0.2182, 'grad_norm': 25.6026668548584, 'learning_rate': 1.6654804270462634e-05, 'epoch': 0.44483985765124556}
{'loss': 0.1946, 'grad_norm': 11.42795467376709, 'learning_rate': 1.3318505338078291e-05, 'epoch': 0.556049822064057}
{'loss': 0.1738, 'grad_norm': 2.092336654663086, 'learning_rate': 9.98220640569395e-06, 'epoch': 0.6672597864768683}
{'loss': 0.1577, 'grad_norm': 24.28145408630371, 'learning_rate': 6.645907473309609e-06, 'epoch': 0.7784697508896797}
{'loss': 0.1259, 'grad_norm': 0.026618866249918938, 'learning_rate': 3.309608540925267e-06, 'epoch': 0.8896797153024911}
{'eval_loss': 0.43768376111984253, 'eval_runtime': 931.2215, 'eval_samples_per_second': 181.349, 'eval_steps_per_second': 0.709, 'epoch': 1.0}
{'train_runtime': 2130.7656, 'train_samples_per_second': 33.755, 'train_steps_per_second': 2.11, 'train_loss': 0.26160650321173073, 'epoch': 1.0}
Post-processing 1880518 example predictions split into 1893865 features.
{
    "68": {
        "QA": {
            "exact_match": 90.88435046481383,
            "f1": 96.58216385349634
        },
        "PR": {
            "p@1": 0.963766196233322,
            "p@2": 0.9919982178122441,
            "p@3": 0.9962008092095757
        },
        "PRQA": {
            "exact_match": 88.60844853330764,
            "f1": 94.45309752461011
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1200240 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1200240 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
