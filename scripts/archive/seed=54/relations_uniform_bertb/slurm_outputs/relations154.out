2024-05-08 00:18:08 INFO     ------------- Experiment: model BERTbase, frequency threshold 154 ---------------
2024-05-08 00:18:08 INFO     Contexts were splited into 716 paragraphs, which are 2.402684563758389 paragraphs on average per one report. The overall paragraph average length (characters) is 2310.9930167597763
2024-05-08 00:18:08 INFO     Contexts were splited into 79 paragraphs, which are 1.880952380952381 paragraphs on average per one report. The overall paragraph average length (characters) is 2278.4430379746836
2024-05-08 00:18:09 INFO     Contexts were splited into 188 paragraphs, which are 2.186046511627907 paragraphs on average per one report. The overall paragraph average length (characters) is 2243.494680851064
2024-05-08 00:18:36 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-08 00:24:42 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-08 01:59:53 INFO     the model is trained
2024-05-08 02:20:41 INFO     evaluation data are prepared
2024-05-08 06:36:13 INFO     QA scores: {'exact_match': 91.28654689080487, 'f1': 96.13434793819411}
2024-05-08 06:36:13 INFO     PR scores: {'p@1': 0.9849477385482395, 'p@2': 0.9964958335340302, 'p@3': 0.9987416309426328}
2024-05-08 06:36:23 INFO     PRQA scores: {'exact_match': 90.2846683685757, 'f1': 95.26266159325999}
{'loss': 0.6696, 'grad_norm': 16.073057174682617, 'learning_rate': 2.874476987447699e-05, 'epoch': 0.04184100418410042}
{'loss': 0.3543, 'grad_norm': 0.1443902552127838, 'learning_rate': 2.7489539748953977e-05, 'epoch': 0.08368200836820083}
{'loss': 0.2851, 'grad_norm': 13.710108757019043, 'learning_rate': 2.6234309623430962e-05, 'epoch': 0.12552301255230125}
{'loss': 0.2591, 'grad_norm': 20.564319610595703, 'learning_rate': 2.497907949790795e-05, 'epoch': 0.16736401673640167}
{'loss': 0.2277, 'grad_norm': 2.3719558715820312, 'learning_rate': 2.372384937238494e-05, 'epoch': 0.20920502092050208}
{'loss': 0.2205, 'grad_norm': 15.957080841064453, 'learning_rate': 2.2468619246861927e-05, 'epoch': 0.2510460251046025}
{'loss': 0.1958, 'grad_norm': 4.82879114151001, 'learning_rate': 2.1213389121338912e-05, 'epoch': 0.2928870292887029}
{'loss': 0.196, 'grad_norm': 7.696127414703369, 'learning_rate': 1.99581589958159e-05, 'epoch': 0.33472803347280333}
{'loss': 0.1676, 'grad_norm': 11.90513801574707, 'learning_rate': 1.870292887029289e-05, 'epoch': 0.37656903765690375}
{'loss': 0.158, 'grad_norm': 3.221717357635498, 'learning_rate': 1.7447698744769877e-05, 'epoch': 0.41841004184100417}
{'loss': 0.1586, 'grad_norm': 21.077320098876953, 'learning_rate': 1.6192468619246862e-05, 'epoch': 0.4602510460251046}
{'loss': 0.1443, 'grad_norm': 1.6936734914779663, 'learning_rate': 1.493723849372385e-05, 'epoch': 0.502092050209205}
{'loss': 0.1183, 'grad_norm': 0.915401041507721, 'learning_rate': 1.3682008368200839e-05, 'epoch': 0.5439330543933054}
{'loss': 0.1393, 'grad_norm': 8.907198905944824, 'learning_rate': 1.2426778242677825e-05, 'epoch': 0.5857740585774058}
{'loss': 0.1237, 'grad_norm': 13.422630310058594, 'learning_rate': 1.1171548117154812e-05, 'epoch': 0.6276150627615062}
{'loss': 0.1095, 'grad_norm': 7.512768268585205, 'learning_rate': 9.916317991631798e-06, 'epoch': 0.6694560669456067}
{'loss': 0.1049, 'grad_norm': 23.534109115600586, 'learning_rate': 8.661087866108787e-06, 'epoch': 0.7112970711297071}
{'loss': 0.0992, 'grad_norm': 12.245587348937988, 'learning_rate': 7.405857740585774e-06, 'epoch': 0.7531380753138075}
{'loss': 0.0801, 'grad_norm': 1.9870349168777466, 'learning_rate': 6.150627615062762e-06, 'epoch': 0.7949790794979079}
{'loss': 0.0876, 'grad_norm': 25.03380012512207, 'learning_rate': 4.895397489539749e-06, 'epoch': 0.8368200836820083}
{'loss': 0.0975, 'grad_norm': 4.647421836853027, 'learning_rate': 3.6401673640167366e-06, 'epoch': 0.8786610878661087}
{'loss': 0.0849, 'grad_norm': 23.086624145507812, 'learning_rate': 2.384937238493724e-06, 'epoch': 0.9205020920502092}
{'loss': 0.0817, 'grad_norm': 0.055559951812028885, 'learning_rate': 1.1297071129707113e-06, 'epoch': 0.9623430962343096}
{'eval_loss': 0.3241780698299408, 'eval_runtime': 2456.9457, 'eval_samples_per_second': 178.51, 'eval_steps_per_second': 0.698, 'epoch': 1.0}
{'train_runtime': 5709.2202, 'train_samples_per_second': 33.488, 'train_steps_per_second': 2.093, 'train_loss': 0.17704730109689626, 'epoch': 1.0}
Post-processing 524311 example predictions split into 1485611 features.
{
    "154": {
        "QA": {
            "exact_match": 91.28654689080487,
            "f1": 96.13434793819411
        },
        "PR": {
            "p@1": 0.9849477385482395,
            "p@2": 0.9964958335340302,
            "p@3": 0.9987416309426328
        },
        "PRQA": {
            "exact_match": 90.2846683685757,
            "f1": 95.26266159325999
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 481430 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 481430 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
