2024-05-07 18:05:22 INFO     ------------- Experiment: model BERTbase, frequency threshold 77 ---------------
2024-05-07 18:05:22 INFO     Contexts were splited into 1288 paragraphs, which are 7.038251366120218 paragraphs on average per one report. The overall paragraph average length (characters) is 934.2197204968944
2024-05-07 18:05:22 INFO     Contexts were splited into 194 paragraphs, which are 7.461538461538462 paragraphs on average per one report. The overall paragraph average length (characters) is 933.5773195876288
2024-05-07 18:05:23 INFO     Contexts were splited into 361 paragraphs, which are 6.811320754716981 paragraphs on average per one report. The overall paragraph average length (characters) is 939.0083102493074
2024-05-07 18:05:31 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-07 18:06:55 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-07 18:33:38 INFO     the model is trained
2024-05-07 18:38:50 INFO     evaluation data are prepared
2024-05-07 19:50:21 INFO     QA scores: {'exact_match': 29.58206262617585, 'f1': 72.00435618455485}
2024-05-07 19:50:21 INFO     PR scores: {'p@1': 0.8680469052016666, 'p@2': 0.9678922726686998, 'p@3': 0.9881663158799021}
2024-05-07 19:50:24 INFO     PRQA scores: {'exact_match': 27.68137107512564, 'f1': 66.8726712214688}
{'loss': 1.9166, 'grad_norm': 18.75418472290039, 'learning_rate': 2.6992782678428228e-05, 'epoch': 0.10024057738572574}
{'loss': 1.2328, 'grad_norm': 16.022689819335938, 'learning_rate': 2.398556535685646e-05, 'epoch': 0.20048115477145148}
{'loss': 1.0277, 'grad_norm': 19.825111389160156, 'learning_rate': 2.0978348035284682e-05, 'epoch': 0.30072173215717724}
{'loss': 0.8628, 'grad_norm': 16.384870529174805, 'learning_rate': 1.797113071371291e-05, 'epoch': 0.40096230954290296}
{'loss': 0.7292, 'grad_norm': 11.701739311218262, 'learning_rate': 1.496391339214114e-05, 'epoch': 0.5012028869286287}
{'loss': 0.6528, 'grad_norm': 13.123637199401855, 'learning_rate': 1.1956696070569367e-05, 'epoch': 0.6014434643143545}
{'loss': 0.5592, 'grad_norm': 15.661688804626465, 'learning_rate': 8.949478748997594e-06, 'epoch': 0.7016840417000801}
{'loss': 0.5081, 'grad_norm': 19.15244483947754, 'learning_rate': 5.942261427425823e-06, 'epoch': 0.8019246190858059}
{'loss': 0.4452, 'grad_norm': 21.495445251464844, 'learning_rate': 2.93504410585405e-06, 'epoch': 0.9021651964715317}
{'eval_loss': 1.7766118049621582, 'eval_runtime': 262.0003, 'eval_samples_per_second': 180.034, 'eval_steps_per_second': 0.706, 'epoch': 1.0}
{'train_runtime': 1601.7385, 'train_samples_per_second': 49.82, 'train_steps_per_second': 3.114, 'train_loss': 0.8370487057120684, 'epoch': 1.0}
Post-processing 354547 example predictions split into 415153 features.
{
    "77": {
        "QA": {
            "exact_match": 29.58206262617585,
            "f1": 72.00435618455485
        },
        "PR": {
            "p@1": 0.8680469052016666,
            "p@2": 0.9678922726686998,
            "p@3": 0.9881663158799021
        },
        "PRQA": {
            "exact_match": 27.68137107512564,
            "f1": 66.8726712214688
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1200124 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1200124 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
