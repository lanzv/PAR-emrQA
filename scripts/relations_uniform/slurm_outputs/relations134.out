2024-05-09 17:02:03 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 134 ---------------
2024-05-09 17:02:03 INFO     Contexts were splited into 1556 paragraphs, which are 5.221476510067114 paragraphs on average per one report. The overall paragraph average length (characters) is 1063.4132390745501
2024-05-09 17:02:03 INFO     Contexts were splited into 171 paragraphs, which are 4.071428571428571 paragraphs on average per one report. The overall paragraph average length (characters) is 1052.6140350877192
2024-05-09 17:02:04 INFO     Contexts were splited into 397 paragraphs, which are 4.616279069767442 paragraphs on average per one report. The overall paragraph average length (characters) is 1062.410579345088
2024-05-09 17:02:33 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-09 17:05:30 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-09 17:42:58 INFO     the model is trained
2024-05-09 18:03:38 INFO     evaluation data are prepared
2024-05-09 21:46:02 INFO     QA scores: {'exact_match': 91.56471268243341, 'f1': 97.27322894326143}
2024-05-09 21:46:02 INFO     PR scores: {'p@1': 0.9829367564182843, 'p@2': 0.9969714849959058, 'p@3': 0.9989764462212802}
2024-05-09 21:46:13 INFO     PRQA scores: {'exact_match': 90.364144309041, 'f1': 96.19858271038521}
{'loss': 0.7222, 'grad_norm': 12.117332458496094, 'learning_rate': 2.67982924226254e-05, 'epoch': 0.10672358591248667}
{'loss': 0.2602, 'grad_norm': 1.6186937093734741, 'learning_rate': 2.35965848452508e-05, 'epoch': 0.21344717182497333}
{'loss': 0.1922, 'grad_norm': 3.6109609603881836, 'learning_rate': 2.0394877267876203e-05, 'epoch': 0.32017075773745995}
{'loss': 0.1586, 'grad_norm': 13.81376838684082, 'learning_rate': 1.7193169690501603e-05, 'epoch': 0.42689434364994666}
{'loss': 0.1353, 'grad_norm': 0.22733858227729797, 'learning_rate': 1.3991462113127e-05, 'epoch': 0.5336179295624333}
{'loss': 0.1177, 'grad_norm': 0.5559143424034119, 'learning_rate': 1.0789754535752402e-05, 'epoch': 0.6403415154749199}
{'loss': 0.1006, 'grad_norm': 0.9215855002403259, 'learning_rate': 7.588046958377802e-06, 'epoch': 0.7470651013874067}
{'loss': 0.1089, 'grad_norm': 29.08958625793457, 'learning_rate': 4.386339381003202e-06, 'epoch': 0.8537886872998933}
{'loss': 0.0857, 'grad_norm': 1.501983404159546, 'learning_rate': 1.1846318036286021e-06, 'epoch': 0.96051227321238}
{'eval_loss': 0.36136552691459656, 'eval_runtime': 990.9298, 'eval_samples_per_second': 179.824, 'eval_steps_per_second': 0.703, 'epoch': 1.0}
{'train_runtime': 2247.2308, 'train_samples_per_second': 33.35, 'train_steps_per_second': 2.085, 'train_loss': 0.20454132992022828, 'epoch': 1.0}
Post-processing 1164354 example predictions split into 1276345 features.
{
    "134": {
        "QA": {
            "exact_match": 91.56471268243341,
            "f1": 97.27322894326143
        },
        "PR": {
            "p@1": 0.9829367564182843,
            "p@2": 0.9969714849959058,
            "p@3": 0.9989764462212802
        },
        "PRQA": {
            "exact_match": 90.364144309041,
            "f1": 96.19858271038521
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 1207274 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 1207274 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
