2024-05-09 23:03:02 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 157 ---------------
2024-05-09 23:03:02 INFO     Contexts were splited into 587 paragraphs, which are 1.9697986577181208 paragraphs on average per one report. The overall paragraph average length (characters) is 2818.8603066439523
2024-05-09 23:03:03 INFO     Contexts were splited into 67 paragraphs, which are 1.5952380952380953 paragraphs on average per one report. The overall paragraph average length (characters) is 2686.5223880597014
2024-05-09 23:03:04 INFO     Contexts were splited into 151 paragraphs, which are 1.755813953488372 paragraphs on average per one report. The overall paragraph average length (characters) is 2793.225165562914
2024-05-09 23:03:33 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-09 23:10:52 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-10 00:54:40 INFO     the model is trained
2024-05-10 01:15:56 INFO     evaluation data are prepared
2024-05-10 05:15:46 INFO     QA scores: {'exact_match': 91.59842974808535, 'f1': 97.25737442671128}
2024-05-10 05:15:46 INFO     PR scores: {'p@1': 0.9896621068349308, 'p@2': 0.9983924184769519, 'p@3': 0.9997651847213526}
2024-05-10 05:15:56 INFO     PRQA scores: {'exact_match': 90.83076441404556, 'f1': 96.60868152773618}
{'loss': 0.6523, 'grad_norm': 3.890421152114868, 'learning_rate': 2.887883997309216e-05, 'epoch': 0.03737200089692802}
{'loss': 0.2433, 'grad_norm': 6.807555198669434, 'learning_rate': 2.7757679946184316e-05, 'epoch': 0.07474400179385604}
{'loss': 0.1884, 'grad_norm': 7.744750022888184, 'learning_rate': 2.663651991927648e-05, 'epoch': 0.11211600269078406}
{'loss': 0.1654, 'grad_norm': 1.002543568611145, 'learning_rate': 2.551535989236864e-05, 'epoch': 0.14948800358771208}
{'loss': 0.1512, 'grad_norm': 78.60379791259766, 'learning_rate': 2.4394199865460798e-05, 'epoch': 0.1868600044846401}
{'loss': 0.1338, 'grad_norm': 0.0660211592912674, 'learning_rate': 2.3273039838552954e-05, 'epoch': 0.22423200538156812}
{'loss': 0.1401, 'grad_norm': 2.3529014587402344, 'learning_rate': 2.2151879811645117e-05, 'epoch': 0.26160400627849617}
{'loss': 0.1256, 'grad_norm': 18.757436752319336, 'learning_rate': 2.1030719784737276e-05, 'epoch': 0.29897600717542416}
{'loss': 0.1166, 'grad_norm': 6.2417473793029785, 'learning_rate': 1.9909559757829435e-05, 'epoch': 0.3363480080723522}
{'loss': 0.1049, 'grad_norm': 14.46845817565918, 'learning_rate': 1.878839973092159e-05, 'epoch': 0.3737200089692802}
{'loss': 0.0975, 'grad_norm': 18.213682174682617, 'learning_rate': 1.7667239704013754e-05, 'epoch': 0.41109200986620825}
{'loss': 0.0991, 'grad_norm': 13.633549690246582, 'learning_rate': 1.6546079677105914e-05, 'epoch': 0.44846401076313624}
{'loss': 0.0868, 'grad_norm': 0.9094141125679016, 'learning_rate': 1.5424919650198073e-05, 'epoch': 0.4858360116600643}
{'loss': 0.0747, 'grad_norm': 2.1985912322998047, 'learning_rate': 1.4303759623290232e-05, 'epoch': 0.5232080125569923}
{'loss': 0.085, 'grad_norm': 0.03876333683729172, 'learning_rate': 1.318259959638239e-05, 'epoch': 0.5605800134539203}
{'loss': 0.0796, 'grad_norm': 1.0083563327789307, 'learning_rate': 1.2061439569474551e-05, 'epoch': 0.5979520143508483}
{'loss': 0.0696, 'grad_norm': 0.13564522564411163, 'learning_rate': 1.0940279542566709e-05, 'epoch': 0.6353240152477764}
{'loss': 0.0645, 'grad_norm': 6.484494209289551, 'learning_rate': 9.81911951565887e-06, 'epoch': 0.6726960161447044}
{'loss': 0.0651, 'grad_norm': 0.008596486411988735, 'learning_rate': 8.697959488751028e-06, 'epoch': 0.7100680170416324}
{'loss': 0.0618, 'grad_norm': 1.0089571475982666, 'learning_rate': 7.576799461843188e-06, 'epoch': 0.7474400179385604}
{'loss': 0.0722, 'grad_norm': 0.035186223685741425, 'learning_rate': 6.4556394349353465e-06, 'epoch': 0.7848120188354885}
{'loss': 0.0617, 'grad_norm': 7.901216983795166, 'learning_rate': 5.334479408027506e-06, 'epoch': 0.8221840197324165}
{'loss': 0.056, 'grad_norm': 2.391037940979004, 'learning_rate': 4.213319381119665e-06, 'epoch': 0.8595560206293444}
{'loss': 0.0481, 'grad_norm': 0.15386323630809784, 'learning_rate': 3.0921593542118247e-06, 'epoch': 0.8969280215262725}
{'loss': 0.0493, 'grad_norm': 0.033260270953178406, 'learning_rate': 1.970999327303984e-06, 'epoch': 0.9343000224232005}
{'loss': 0.0503, 'grad_norm': 0.005853781942278147, 'learning_rate': 8.498393003961433e-07, 'epoch': 0.9716720233201286}
{'eval_loss': 0.26178133487701416, 'eval_runtime': 2640.7178, 'eval_samples_per_second': 179.92, 'eval_steps_per_second': 0.703, 'epoch': 1.0}
{'train_runtime': 6226.8153, 'train_samples_per_second': 34.377, 'train_steps_per_second': 2.149, 'train_loss': 0.11887125203014089, 'epoch': 1.0}
Post-processing 402318 example predictions split into 1406525 features.
{
    "157": {
        "QA": {
            "exact_match": 91.59842974808535,
            "f1": 97.25737442671128
        },
        "PR": {
            "p@1": 0.9896621068349308,
            "p@2": 0.9983924184769519,
            "p@3": 0.9997651847213526
        },
        "PRQA": {
            "exact_match": 90.83076441404556,
            "f1": 96.60868152773618
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 704161 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 704161 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
