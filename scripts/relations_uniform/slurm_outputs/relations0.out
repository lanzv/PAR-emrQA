2024-05-10 15:19:47 INFO     ------------- Experiment: model ClinicalBERT, frequency threshold 0 ---------------
2024-05-10 15:19:48 INFO     Contexts were splited into 6517 paragraphs, which are 21.869127516778523 paragraphs on average per one report. The overall paragraph average length (characters) is 253.90072119073193
2024-05-10 15:19:48 INFO     Contexts were splited into 710 paragraphs, which are 16.904761904761905 paragraphs on average per one report. The overall paragraph average length (characters) is 253.5169014084507
2024-05-10 15:19:50 INFO     Contexts were splited into 1666 paragraphs, which are 19.372093023255815 paragraphs on average per one report. The overall paragraph average length (characters) is 253.16746698679472
2024-05-10 15:20:36 INFO     datasets are converted to Datset format
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../models/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-05-10 15:22:17 INFO     training data are prepared
/home/lanz/.local/lib/python3.10/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2024-05-10 15:58:39 INFO     the model is trained
2024-05-10 16:39:21 INFO     evaluation data are prepared
2024-05-11 06:18:38 INFO     QA scores: {'exact_match': 93.16446702952652, 'f1': 98.28889032238526}
2024-05-11 06:18:39 INFO     PR scores: {'p@1': 0.9711899715813304, 'p@2': 0.995604739656086, 'p@3': 0.9984165020952748}
2024-05-11 06:18:49 INFO     PRQA scores: {'exact_match': 90.6471268243341, 'f1': 96.16495806459636}
{'loss': 0.5347, 'grad_norm': 6.458846092224121, 'learning_rate': 2.6703296703296702e-05, 'epoch': 0.10989010989010989}
{'loss': 0.1619, 'grad_norm': 3.22502064704895, 'learning_rate': 2.3406593406593407e-05, 'epoch': 0.21978021978021978}
{'loss': 0.1229, 'grad_norm': 0.8597811460494995, 'learning_rate': 2.010989010989011e-05, 'epoch': 0.32967032967032966}
{'loss': 0.0977, 'grad_norm': 0.0638747587800026, 'learning_rate': 1.6813186813186814e-05, 'epoch': 0.43956043956043955}
{'loss': 0.094, 'grad_norm': 13.719752311706543, 'learning_rate': 1.3516483516483517e-05, 'epoch': 0.5494505494505495}
{'loss': 0.0747, 'grad_norm': 0.12853406369686127, 'learning_rate': 1.021978021978022e-05, 'epoch': 0.6593406593406593}
{'loss': 0.0651, 'grad_norm': 3.7937304973602295, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.7692307692307693}
{'loss': 0.0719, 'grad_norm': 0.9870400428771973, 'learning_rate': 3.6263736263736266e-06, 'epoch': 0.8791208791208791}
{'loss': 0.0571, 'grad_norm': 0.021063482388854027, 'learning_rate': 3.296703296703297e-07, 'epoch': 0.989010989010989}
{'eval_loss': 0.2810227870941162, 'eval_runtime': 962.1528, 'eval_samples_per_second': 179.011, 'eval_steps_per_second': 0.699, 'epoch': 1.0}
{'train_runtime': 2181.3876, 'train_samples_per_second': 33.367, 'train_steps_per_second': 2.086, 'train_loss': 0.14125869095980467, 'epoch': 1.0}
Post-processing 4927331 example predictions split into 4927331 features.
{
    "0": {
        "QA": {
            "exact_match": 93.16446702952652,
            "f1": 98.28889032238526
        },
        "PR": {
            "p@1": 0.9711899715813304,
            "p@2": 0.995604739656086,
            "p@3": 0.9984165020952748
        },
        "PRQA": {
            "exact_match": 90.6471268243341,
            "f1": 96.16495806459636
        }
    }
}
slurmstepd: error: common_file_write_uint32s: write pid 500470 to /sys/fs/cgroup/cgroup.procs failed: Device or resource busy
slurmstepd: error: Unable to move pid 500470 to init root cgroup /sys/fs/cgroup
slurmstepd: error: error unlocking cgroup '/sys/fs/cgroup/system.slice/slurmstepd.scope' : Bad file descriptor
